<!DOCTYPE html>
<html lang="en">
<head>
    <title>DiagnostiX.ai | Artificial Intelligence for Radiology</title>

    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Title -->

    <meta name="twitter:title" content="DiagnostiX.ai | A.I. Powered X-Ray Analysis">
    <meta name="og:title" content="DiagnostiX.ai | A.I. Powered X-Ray Analysis">


    <!-- Description -->

    <meta name="description"
          content="DiagnostiX.ai develops deep learning algorithms that interpret radiology images. Our mission is to make healthcare accessible and affordable using the power of deep learning.">
    <meta name="twitter:description"
          content="DiagnostiX.ai develops deep learning algorithms that interpret radiology images. Our mission is to make healthcare accessible and affordable using the power of deep learning.">
    <meta property="og:description"
          content="DiagnostiX.ai develops deep learning algorithms that interpret radiology images. Our mission is to make healthcare accessible and affordable using the power of deep learning.">


    <!-- Image -->

    <meta property="og:image" content="images/DiagnostiX_ai_AI_for_radiology.png">
    <meta name="twitter:image" content="http://DiagnostiX.ai/assets/images/DiagnostiX_ai_AI_for_radiology.png">


    <meta name="twitter:site" content="@DiagnostiX_ai">
    <meta name="twitter:card" content="summary">
    <meta name="og:url" content="/">
    <meta name="keywords"
          content="deep learning, radiology, healthcare, patient, innovation, research, artificial intelligence, AI for Radiology, CT scan AI, X-ray, Artificial Intelligence, A.I. Powered X-Ray Analysis">


    <link rel="shortcut icon" href="favicon.ico">
    <link href="css/css.css" rel="stylesheet" type="text/css">
    <link href="css/css_1.css" rel="stylesheet" type="text/css">
    <!-- Global CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css" crossorigin="anonymous">
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/flexslider.css">
    <!-- Theme CSS -->
    <link id="theme-style" rel="stylesheet" href="css/styles.css">
    <link id="theme-style" rel="stylesheet" href="css/oldwebsite.css">
    <!-- jquery -->
    <script src="js/jquery-3.3.1.min.js" crossorigin="anonymous"></script>

    <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css">
    <script src="js/cookieconsent.min.js"></script>

    <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css">
    <script src="js/cookieconsent.min.js"></script>
    <script>
        window.addEventListener("load", function () {
            window.cookieconsent.initialise({
                "palette": {
                    "popup": {
                        "background": "#001242"
                    },
                    "button": {
                        "background": "#0094c6"
                    }
                },
                "content": {
                    "href": "/privacy-policy.html"
                }
            })
        });
    </script>

</head>

<body class="blog-page blog-category-page input-delete-news">
<header id="header" class="header navbar-fixed-top">
    <div class="container">

        <!-- Logo -->
        <h1 class="logo">
            <a href="/"><img src="images/header_logo_03.png" alt="logo" class="logo navlogo">
            </a>
        </h1>
        <!--//logo-->

        <!-- Nav Bar -->
        <nav class="main-nav navbar navbar-right navbar-inverse navbar-expand-md" role="navigation">
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapse"
                    aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div id="navbar-collapse" class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                    <li class=" active  nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class=" nav-item dropdown">
                        <a class="nav-link" data-toggle="dropdown" aria-haspopup="true"
                           aria-expanded="false" href="#">Products</a>
                        <!--                            <div class="dropdown-menu dropdown-menu-right">-->
                        <!--                                <h6 class="dropdown-header">In the Clinic</h6>-->
                        <!--                                <a class="dropdown-item" href="/qxr.html">Chest X-rays</a>-->
                        <!--                                <a class="dropdown-item" href="/headct.html">Head CT Scans</a>-->
                        <!--                                <a class="dropdown-item" href="/poqus.html">POqUS</a>-->
                        <!--                                <a class="dropdown-item" href="/qct-lung.html">CHEST CT SCANS</a>-->
                        <!--                                <h2 class="dropdown-header ">In the Community </h2>-->
                        <!--                                <a class="dropdown-item" href="/publichealth.html">Public health solutions</a>-->
                        <!--                                <a class="dropdown-item" href="/qxr-tuberculosis.html">Tuberculosis </a>-->
                        <!--                                <a class="dropdown-item" href="/covid.html">COVID-19 </a>-->
                        <!--                            </div>-->
                    </li>
                    <!--//dropdown-->
                    <li class=" nav-item"><a class="nav-link" href="/about.html">About Us</a></li>
                    <li class=" nav-item dropdown">
                        <a class="nav-link" data-toggle="dropdown" aria-haspopup="true"
                           aria-expanded="false" href="#">News</a>
                        <!--                            <div class="dropdown-menu dropdown-menu-right">-->
                        <!--                                <a class="dropdown-item" href="/news.html">Highlights</a>-->
                        <!--                                <a class="dropdown-item" href="/press_coverage.html">Press coverage</a>-->
                        <!--                            </div>-->
                    </li>
                    <li class="nav-item"><a class="nav-link" target="_blank" href="#">Blog</a>
                    </li>
                    <li class="nav-item nav-item-cta last"><a class="btn btn-cta btn-cta-secondary" target="_blank"
                                                              href="#">Try Now<sup>* </sup></a></li>
                </ul><!--//nav-->
            </div><!--//navabr-collapse-->
        </nav><!--//main-nav-->
    </div><!--//container-->
</header><!--//header-->

<div class="blog blog-category blog-archive container">
    <h2 class="page-title text-center">Publications and Conference Presentations</h2>

    <div class="blog-list blog-category-list">
        <div class="row">
            <div class="mr-lg-auto ml-lg-auto col-12 col-lg-10 mb-4 filter-container">
                <span>Showing: </span>
                <div>
                    <button class="btn active" data-type="all">All</button>
                    <button class="btn" data-type="journal">Journals</button>
                    <button class="btn" data-type="conference">Conference Presentations</button>
                </div>
            </div>

            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-31" class="post-title">Artificial intelligence matches subjective severity
                            assessment of pneumonia for prediction of patient outcome and need for mechanical
                            ventilation - a cohort study</h3>
                        <p class="post-authors">

                            Shadi Ebrahimian<sup>1</sup> ,

                            Fatemeh Homayounieh<sup>1</sup> ,

                            Marcio A. B. C. Rockenbach<sup>2</sup> ,

                            Preetham Putha<sup>3</sup> ,

                            Tarun Raj<sup>3</sup> ,

                            Ittai Dayan<sup>1</sup> ,

                            Bernardo C. Bizzo<sup>1</sup> ,

                            Varun Buch<sup>2</sup> ,

                            Dufan Wu<sup>1</sup> ,

                            Kyungsang Kim<sup>1</sup> ,

                            Quanzheng Li<sup>1</sup> ,

                            Subba R. Digumarthy<sup>1</sup> ,

                            Mannudeep K. Kalra<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Department of Radiology, Massachusetts General Hospital and the Harvard Medical School,
                            75 Blossom Court, Suite 248, Boston, MA, 02114, USA;&nbsp

                            2. MGH & BWH Center for Clinical Data Science, Boston, MA, USA;&nbsp

                            3. Qure.ai, Mumbai, India;&nbsp

                            4. Gordon Center for Medical Imaging, Bartlett 501, 55 Fruit Street, Boston, MA, 02114, USA


                        </p>
                        <a href="/publications/2021/01/13/ai-pneumonia-outcomes.html">
                            <p class="published-date">
                                Published:
                                13 January 2021, Nature Scientific Reports
                            </p>
                        </a>
                        <input type="checkbox" id="post-31">
                        <label class="show-abstract" for="post-31"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="abstract">Abstract</h4>
                            <p>To compare the performance of artificial intelligence (AI) and Radiographic Assessment of
                                Lung Edema (RALE) scores from frontal chest radiographs (CXRs) for predicting patient
                                outcomes and the need for mechanical ventilation in COVID-19 pneumonia. Our IRB-approved
                                study included 1367 serial CXRs from 405 adult patients (mean age 65 ± 16 years) from
                                two sites in the US (Site A) and South Korea (Site B). We recorded information
                                pertaining to patient demographics (age, gender), smoking history, comorbid conditions
                                (such as cancer, cardiovascular and other diseases), vital signs (temperature, oxygen
                                saturation), and available laboratory data (such as WBC count and CRP). Two thoracic
                                radiologists performed the qualitative assessment of all CXRs based on the RALE score
                                for assessing the severity of lung involvement. All CXRs were processed with a
                                commercial AI algorithm to obtain the percentage of the lung affected with findings
                                related to COVID-19 (AI score). Independent t- and chi-square tests were used in
                                addition to multiple logistic regression with Area Under the Curve (AUC) as output for
                                predicting disease outcome and the need for mechanical ventilation. The RALE and AI
                                scores had a strong positive correlation in CXRs from each site (r2 = 0.79–0.86; p &lt; 0.0001).
                                Patients who died or received mechanical ventilation had significantly higher RALE and
                                AI scores than those with recovery or without the need for mechanical ventilation (p &lt; 0.001).
                                Patients with a more substantial difference in baseline and maximum RALE scores and AI
                                scores had a higher prevalence of death and mechanical ventilation (p &lt; 0.001). The
                                addition of patients’ age, gender, WBC count, and peripheral oxygen saturation increased
                                the outcome prediction from 0.87 to 0.94 (95% CI 0.90–0.97) for RALE scores and from
                                0.82 to 0.91 (95% CI 0.87–0.95) for the AI scores. AI algorithm is as robust a predictor
                                of adverse patient outcome (death or need for mechanical ventilation) as subjective RALE
                                scores in patients with COVID-19 pneumonia.</p>

                            <a target="_blank" class="read-more"
                               href="https://www.nature.com/articles/s41598-020-79470-0#author-information"> Read full
                                paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-30" class="post-title">Automated Lateral Ventricular and Cranial Vault
                            Volume Measurements in 13,851 Subjects Utilizing Deep Learning Algorithms</h3>
                        <p class="post-authors">

                            Georgios A Maragkos<sup>1</sup> ,

                            Aristotelis S Filippidis<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>2</sup> ,

                            Mohamed M Salem<sup>1</sup> ,

                            Swetha Tanamala<sup>2</sup> ,

                            Santiago Gomez-Paz<sup>1</sup> ,

                            Pooja Rao<sup>2</sup> ,

                            Justin M Moore<sup>1</sup> ,

                            Efstathios Papavassiliou<sup>1</sup> ,

                            David Hackney<sup>3</sup> ,

                            Ajith J Thomas<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Neurosurgery Service, Beth Israel Deaconess Medical Center, Harvard Medical School,
                            Boston, MA, USA;&nbsp

                            2. Qure.ai, Mumbai, India;&nbsp

                            3. Radiology Department, Beth Israel Deaconess Medical Center, Harvard Medical School,
                            Boston, MA, USA


                        </p>
                        <a href="/publications/2021/01/06/automated-lv-cranial-vault-volume.html">
                            <p class="published-date">
                                Published:
                                06 January 2021, World Neurosurgery
                            </p>
                        </a>
                        <input type="checkbox" id="post-30">
                        <label class="show-abstract" for="post-30"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background">Background</h4>
                            <p>Currently, no large dataset-derived standard has been established for normal or
                                pathologic human cerebral ventricular and cranial vault volumes. Automated volumetric
                                measurements could be used to assist in diagnosis and follow-up of hydrocephalus or
                                craniofacial syndromes. In this work we use deep learning algorithms to measure
                                ventricular and cranial vault volumes in a large dataset of head computed tomography
                                (CT) scans.</p>

                            <h4 id="methods">Methods</h4>
                            <p>A cross-sectional dataset comprising 13,851 CT scans was utilized to deploy U-net deep
                                learning networks to segment and quantify lateral cerebral ventricular and cranial vault
                                volumes in relation to age and sex. The models were validated against manual
                                segmentations. Corresponding radiological reports were annotated using a rule-based
                                natural language processing (NLP) framework to identify normal scans, cerebral atrophy,
                                or hydrocephalus.</p>

                            <h4 id="results">Results</h4>
                            <p>U-net models had high fidelity to manual segmentations for lateral ventricular and
                                cranial vault volume measurements (DICE 0.878 and 0.983, respectively). The NLP
                                identified 6,239 (44.7%) normal radiological reports, 1,827 (13.1%) with cerebral
                                atrophy and 1,185 (8.5%) with hydrocephalus. Age- and sex-based reference tables with
                                medians, 25th and 75th percentiles for scans classified as normal, atrophy and
                                hydrocephalus were constructed. The median lateral ventricular volume in normal scans
                                was significantly smaller compared to hydrocephalus (15.7mL vs 82.0mL, P&lt;0.001).</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>This is the first study to measure lateral ventricular and cranial vault volumes in a
                                large dataset, made possible with artificial intelligence. We provide a robust method to
                                establish normal values for these volumes and a tool to report these on CT scans when
                                evaluating for hydrocephalus.</p>

                            <a target="_blank" class="read-more"
                               href="https://www.sciencedirect.com/science/article/abs/pii/S1878875020327157"> Read full
                                paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-29" class="post-title">Chest x-ray analysis with deep learning-based
                            software as a triage test for pulmonary tuberculosis - a prospective study of diagnostic
                            accuracy for culture-confirmed disease</h3>
                        <p class="post-authors">

                            Faiz Ahmad Khan<sup>1, 2, 3</sup> ,

                            Arman Majidulla<sup>4</sup> ,

                            Gamuchirai Tavaziva<sup>1, 2</sup> ,

                            Ahsana Nazish<sup>5</sup> ,

                            Syed Kumail Abidi<sup>1</sup> ,

                            Andrea Benedetti<sup>2, 3</sup> ,

                            Dick Menzies<sup>1, 2, 3</sup> ,

                            James C Johnston<sup>6</sup> ,

                            Aamir Javed Khan<sup>7</sup> ,

                            Saima Saeed<sup>8</sup>

                        </p>
                        <p class="author-affiliations">

                            1. McGill International TB Centre, Research Institute of the McGill University Health Centre
                            and McGill University Montreal, QC, Canada;&nbsp

                            2. Respiratory Epidemiology and Clinical Research Unit, Centre for Outcomes Research and
                            Evaluation, Research Institute of the McGill University Health Centre, Montreal, QC,
                            Canada;&nbsp

                            3. Department of Medicine and Department of Epidemiology, McGill University, Montreal,
                            Canada;&nbsp

                            4. Interactive Research and Development Pakistan, Karachi, Pakistan;&nbsp

                            5. The Indus Hospital, Karachi, Pakistan;&nbsp

                            6. Ghori TB Clinic, University of British Columbia, Vancouver, BC, Canada;&nbsp

                            7. Interactive Research and Development Global, Singapore;&nbsp

                            8. Global Health Directorate, Indus Health Network, Karachi, Pakistan


                        </p>
                        <a href="/publications/2020/11/01/chest-xray-culture-confirmed-pulmonary-tb.html">
                            <p class="published-date">
                                Published:
                                01 November 2020, The Lancet Digital Health
                            </p>
                        </a>
                        <input type="checkbox" id="post-29">
                        <label class="show-abstract" for="post-29"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background">Background</h4>
                            <p>Deep learning-based radiological image analysis could facilitate use of chest x-rays as
                                triage tests for pulmonary tuberculosis in resource-limited settings. We sought to
                                determine whether commercially available chest x-ray analysis software meet WHO
                                recommendations for minimal sensitivity and specificity as pulmonary tuberculosis triage
                                tests.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We recruited symptomatic adults at the Indus Hospital, Karachi, Pakistan. We compared two
                                software, qXR version 2.0 (qXRv2) and CAD4TB version 6.0 (CAD4TBv6), with a reference of
                                mycobacterial culture of two sputa. We assessed qXRv2 using its manufacturer
                                prespecified threshold score for chest x-ray classification as tuberculosis present
                                versus not present. For CAD4TBv6, we used a data-derived threshold, because it does not
                                have a prespecified one. We tested for non-inferiority to preset WHO recommendations
                                (0·90 for sensitivity, 0·70 for specificity) using a non-inferiority limit of 0·05. We
                                identified factors associated with accuracy by stratification and logistic
                                regression.</p>

                            <h4 id="findings">Findings</h4>
                            <p>We included 2198 (92·7%) of 2370 enrolled participants. 2187 (99·5%) of 2198 were
                                HIV-negative, and 272 (12·4%) had culture-confirmed pulmonary tuberculosis. For both
                                software, accuracy was non-inferior to WHO-recommended minimum values (qXRv2 sensitivity
                                0·93 [95% CI 0·89–0·95], non-inferiority p=0·0002; CAD4TBv6 sensitivity 0·93 [95% CI
                                0·90–0·96], p&lt;0·0001; qXRv2 specificity 0·75 [95% CI 0·73–0·77], p&lt;0·0001;
                                CAD4TBv6 specificity 0·69 [95% CI 0·67–0·71], p=0·0003). Sensitivity was lower in
                                smear-negative pulmonary tuberculosis for both software, and in women for CAD4TBv6.
                                Specificity was lower in men and in those with previous tuberculosis, and reduced with
                                increasing age and decreasing body mass index. Smoking and diabetes did not affect
                                accuracy.</p>

                            <h4 id="interpretation">Interpretation</h4>
                            <p>In an HIV-negative population, these software met WHO-recommended minimal accuracy for
                                pulmonary tuberculosis triage tests. Sensitivity will be lower when smear-negative
                                pulmonary tuberculosis is more prevalent.</p>

                            <a target="_blank" class="read-more"
                               href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30221-1/fulltext#seccestitle10">
                                Read full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-28" class="post-title">Initial chest radiographs and artificial intelligence
                            (AI) predict clinical outcomes in COVID-19 patients - analysis of 697 Italian patients</h3>
                        <p class="post-authors">

                            Junaid Mushtaq<sup>1, 2</sup> ,

                            Renato Pennella<sup>1, 2</sup> ,

                            Salvatore Lavalle<sup>1, 2</sup> ,

                            Anna Colarieti<sup>1, 2</sup> ,

                            Stephanie Steidler<sup>1</sup> ,

                            Carlo M. A. Martinenghi<sup>1</sup> ,

                            Diego Palumbo<sup>1, 2</sup> ,

                            Antonio Esposito<sup>1, 2</sup> ,

                            Patrizia Rovere-Querini<sup>2, 3</sup> ,

                            Moreno Tresoldi<sup></sup> ,

                            Giovanni Landoni<sup>2, 5</sup> ,

                            Fabio Ciceri<sup>2, 6</sup> ,

                            Alberto Zangrillo<sup>2, 5</sup> ,

                            Francesco De Cobelli<sup>1, 2</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Clinical and Experimental Radiology Unit, Experimental Imaging Center, IRCCS San Raffaele
                            Scientific Institute, Milan, Italy;&nbsp

                            2. Faculty of Medicine and Surgery, Vita-Salute San Raffaele University, Via Olgettina 58,
                            Milan, Italy;&nbsp

                            3. Department of Internal Medicine, IRCCS San Raffaele Scientific Institute, Milan,
                            Italy;&nbsp

                            4. Unit of General Medicine and Advanced Care, IRCCS San Raffaele Scientific Institute,
                            Milan, Italy;&nbsp

                            5. Department of Anesthesia and Intensive Care, IRCCS San Raffaele Scientific Institute,
                            Milan, Italy;&nbsp

                            6. Hematology and Bone Marrow Transplantation, IRCCS San Raffaele Scientific Institute,
                            Milan, Italy


                        </p>
                        <a href="/publications/2020/09/18/chest-radiographs-ai-predict-outcomes-in-covid.html">
                            <p class="published-date">
                                Published:
                                18 September 2020, European Radiology
                            </p>
                        </a>
                        <input type="checkbox" id="post-28">
                        <label class="show-abstract" for="post-28"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="objective">Objective</h4>
                            <p>To evaluate whether the initial chest X-ray (CXR) severity assessed by an AI system may
                                have prognostic utility in patients with COVID-19.</p>

                            <h4 id="methods">Methods</h4>
                            <p>This retrospective single-center study included adult patients presenting to the
                                emergency department (ED) between February 25 and April 9, 2020, with SARS-CoV-2
                                infection confirmed on real-time reverse transcriptase polymerase chain reaction
                                (RT-PCR). Initial CXRs obtained on ED presentation were evaluated by a deep learning
                                artificial intelligence (AI) system and compared with the Radiographic Assessment of
                                Lung Edema (RALE) score, calculated by two experienced radiologists. Death and critical
                                COVID-19 (admission to intensive care unit (ICU) or deaths occurring before ICU
                                admission) were identified as clinical outcomes. Independent predictors of adverse
                                outcomes were evaluated by multivariate analyses.</p>

                            <h4 id="results">Results</h4>
                            <p>Six hundred ninety-seven 697 patients were included in the study: 465 males (66.7%),
                                median age of 62 years (IQR 52–75). Multivariate analyses adjusting for demographics and
                                comorbidities showed that an AI system-based score ≥ 30 on the initial CXR was an
                                independent predictor both for mortality (HR 2.60 (95% CI 1.69 − 3.99; p &lt; 0.001))
                                and critical COVID-19 (HR 3.40 (95% CI 2.35–4.94; p &lt; 0.001)). Other independent
                                predictors were RALE score, older age, male sex, coronary artery disease, COPD, and
                                neurodegenerative disease.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>AI- and radiologist-assessed disease severity scores on CXRs obtained on ED presentation
                                were independent and comparable predictors of adverse outcomes in patients with
                                COVID-19.</p>

                            <a target="_blank" class="read-more" href="https://doi.org/10.1007/s00330-020-07269-8"> Read
                                full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-27" class="post-title">Can artificial intelligence (AI) be used to
                            accurately detect tuberculosis (TB) from chest x-ray? A multiplatform evaluation of five AI
                            products used for TB screening in a high TB-burden setting.</h3>
                        <p class="post-authors">

                            Zhi Zhen Qin<sup>1</sup> ,

                            Shahriar Ahmed<sup>2</sup> ,

                            Mohammad Shahnewaz Sarker<sup>2</sup> ,

                            Kishor Paul<sup>2</sup> ,

                            Ahammad Shafiq Sikder Adel<sup>2</sup> ,

                            Tasneem Naheyan<sup>1</sup> ,

                            Sayera Banu<sup>2</sup> ,

                            Jacob Creswell<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Stop TB Partnership, Geneva, Switzerland;&nbsp

                            2. International Centre for Diarrhoeal Disease Research, Bangladesh, Dhaka, Bangladesh


                        </p>
                        <a href="/publications/2020/06/06/can-ai-detect-tb-from-chest-radiograph.html">
                            <p class="published-date">
                                Published:
                                06 June 2020, arxiv
                            </p>
                        </a>
                        <input type="checkbox" id="post-27">
                        <label class="show-abstract" for="post-27"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <p>Powered by artificial intelligence (AI), particularly deep neural networks, computer
                                aided detection (CAD) tools can be trained to recognize TB-related abnormalities on
                                chest radiographs, thereby screening large numbers of people and reducing the pressure
                                on healthcare professionals. Addressing the lack of studies comparing the performance of
                                different products, we evaluated five AI software platforms specific to TB: CAD4TB (v6),
                                InferReadDR (v2), Lunit INSIGHT for Chest Radiography (v4.9.0) , JF CXR-1 (v2) by and
                                qXR (v3) by on an unseen dataset of chest X-rays collected in three TB screening center
                                in Dhaka, Bangladesh. The 23,566 individuals included in the study all received a CXR
                                read by a group of three Bangladeshi board-certified radiologists. A sample of CXRs were
                                re-read by US board-certified radiologists. Xpert was used as the reference standard.
                                All five AI platforms significantly outperformed the human readers. The areas under the
                                receiver operating characteristic curves are qXR: 0.91 (95% CI:0.90-0.91), Lunit INSIGHT
                                CXR: 0.89 (95% CI:0.88-0.89), InferReadDR: 0.85 (95% CI:0.84-0.86), JF CXR-1: 0.85 (95%
                                CI:0.84-0.85), CAD4TB: 0.82 (95% CI:0.81-0.83). We also proposed a new analytical
                                framework that evaluates a screening and triage test and informs threshold selection
                                through tradeoff between cost efficiency and ability to triage. Further, we assessed the
                                performance of the five AI algorithms across the subgroups of age, use cases, and prior
                                TB history, and found that the threshold scores performed differently across different
                                subgroups. The positive results of our evaluation indicate that these AI products can be
                                useful screening and triage tools for active case finding in high TB-burden regions.</p>


                            <a target="_blank" class="read-more" href="https://arxiv.org/abs/2006.05509"> Read full
                                paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-26" class="post-title">Performance of Qure.ai automatic classifiers against
                            a large annotated database of patients with diverse forms of tuberculosis</h3>
                        <p class="post-authors">

                            Eric Engle<sup>1</sup> ,

                            Andrei Gabrielian<sup>1</sup> ,

                            Alyssa Long<sup>1</sup> ,

                            Darrell E. Hurt<sup>1</sup> ,

                            Alex Rosenthal<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Office of Cyber Infrastructure & Computational Biology, National Institute of Allergy and
                            Infectious Disease, National Institutes of Health, Bethesda, MD, United States of America


                        </p>
                        <a href="/publications/2020/01/24/performance-of-qure-ai-automatic-classifiers-tuberculosis.html">
                            <p class="published-date">
                                Published:
                                24 January 2020, PLoS ONE
                            </p>
                        </a>
                        <input type="checkbox" id="post-26">
                        <label class="show-abstract" for="post-26"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <p>Availability of trained radiologists for fast processing of CXRs in regions burdened with
                                tuberculosis always has been a challenge, affecting both timely diagnosis and patient
                                monitoring. The paucity of annotated images of lungs of TB patients hampers attempts to
                                apply data-oriented algorithms for research and clinical practices. The TB Portals
                                Program database (TBPP, https://TBPortals.niaid.nih.gov) is a global collaboration
                                curating a large collection of the most dangerous, hard-to-cure drug-resistant
                                tuberculosis (DR-TB) patient cases. TBPP, with 1,179 (83%) DR-TB patient cases, is a
                                unique collection that is well positioned as a testing ground for deep learning
                                classifiers. As of January 2019, the TBPP database contains 1,538 CXRs, of which 346
                                (22.5%) are annotated by a radiologist and 104 (6.7%) by a pulmonologist–leaving 1,088
                                (70.7%) CXRs without annotations. The Qure.ai qXR artificial intelligence automated CXR
                                interpretation tool, was blind-tested on the 346 radiologist-annotated CXRs from the
                                TBPP database. Qure.ai qXR CXR predictions for cavity, nodule, pleural effusion, hilar
                                lymphadenopathy was successfully matching human expert annotations. In addition, we
                                tested the 12 Qure.ai classifiers to find whether they correlate with treatment success
                                (information provided by treating physicians). Ten descriptors were found as
                                significant: abnormal CXR (p = 0.0005), pleural effusion (p = 0.048), nodule (p =
                                0.0004), hilar lymphadenopathy (p = 0.0038), cavity (p = 0.0002), opacity (p = 0.0006),
                                atelectasis (p = 0.0074), consolidation (p = 0.0004), indicator of TB disease (p = &lt;
                                .0001), and fibrosis (p = &lt; .0001). We conclude that applying fully automated Qure.ai
                                CXR analysis tool is useful for fast, accurate, uniform, large-scale CXR annotation
                                assistance, as it performed well even for DR-TB cases that were not used for initial
                                training. Testing artificial intelligence algorithms (encapsulating both machine
                                learning and deep learning classifiers) on diverse data collections, such as TBPP, is
                                critically important toward progressing to clinically adopted automatic assistants for
                                medical data analysis.</p>


                            <a target="_blank" class="read-more" href="https://doi.org/10.1371/journal.pone.0224445">
                                Read full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-25" class="post-title">Deep learning, computer-aided radiography reading for
                            tuberculosis - A diagnostic accuracy study from a tertiary hospital in India</h3>
                        <p class="post-authors">

                            Madlen Nash<sup>1, 2</sup> ,

                            Rajagopal Kadavigere'<sup>3</sup> ,

                            Jasbon Andrade<sup>3</sup> ,

                            Cynthia Amrutha Sukumar<sup>4</sup> ,

                            Kiran Chawla<sup>5</sup> ,

                            Vishnu Prasad Shenoy<sup>5</sup> ,

                            Tripti Pande<sup>2</sup> ,

                            Sophie Huddart<sup>1, 2</sup> ,

                            Madhukar Pai<sup>1, 2, 7</sup> ,

                            Kavitha Saravu<sup>6, 7</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Department of Epidemiology, Biostatistics and Occupational Health, McGill University,
                            Montreal, Canada;&nbsp

                            2. McGill International TB Centre, McGill University, Montreal, Canada;&nbsp

                            3. Department of Radiodiagnosis, Kasturba Medical College, Manipal, Manipal Academy of
                            Higher Education, Manipal, India;&nbsp

                            4. Department of Medicine, Kasturba Medical College, Manipal, Manipal Academy of Higher
                            Education, Manipal, India;&nbsp

                            5. Department of Microbiology, Kasturba Medical College, Manipal, Manipal Academy of Higher
                            Education, Manipal, India - - Department of Infectious Diseases, Kasturba Medical College,
                            Manipal, Manipal Academy of Higher Education, Manipal, India;&nbsp

                            6. Manipal McGill Program for Infectious Diseases, Manipal Centre for Infectious Diseases,
                            Prasanna School of Public Health, Manipal Academy of Higher Education, Manipal, India


                        </p>
                        <a href="/publications/2020/01/14/deep-learning-computer-aided-tuberculosis-diagnostic-study-tertiary-hospital.html">
                            <p class="published-date">
                                Published:
                                14 January 2020, Nature Scientific Reports
                            </p>
                        </a>
                        <input type="checkbox" id="post-25">
                        <label class="show-abstract" for="post-25"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <p>In general, chest radiographs (CXR) have high sensitivity and moderate specificity for
                                active pulmonary tuberculosis (PTB) screening when interpreted by human readers.
                                However, they are challenging to scale due to hardware costs and the dearth of
                                professionals available to interpret CXR in low-resource, high PTB burden settings.
                                Recently, several computer-aided detection (CAD) programs have been developed to
                                facilitate automated CXR interpretation. We conducted a retrospective case-control study
                                to assess the diagnostic accuracy of a CAD software (qXR, Qure.ai, Mumbai, India) using
                                microbiologically-confirmed PTB as the reference standard. To assess overall accuracy of
                                qXR, receiver operating characteristic (ROC) analysis was used to determine the area
                                under the curve (AUC), along with 95% confidence intervals (CI). Kappa coefficients, and
                                associated 95% CI, were used to investigate inter-rater reliability of the radiologists
                                for detection of specific chest abnormalities. In total, 317 cases and 612 controls were
                                included in the analysis. The AUC for qXR for the detection of
                                microbiologically-confirmed PTB was 0.81 (95% CI: 0.78, 0.84). Using the threshold that
                                maximized sensitivity and specificity of qXR simultaneously, the software achieved a
                                sensitivity and specificity of 71% (95% CI: 66%, 76%) and 80% (95% CI: 77%, 83%),
                                respectively. The sensitivity and specificity of radiologists for the detection of
                                microbiologically-confirmed PTB was 56% (95% CI: 50%, 62%) and 80% (95% CI: 77%, 83%),
                                respectively. For detection of key PTB-related abnormalities ‘pleural effusion’ and
                                ‘cavity’, qXR achieved an AUC of 0.94 (95% CI: 0.92, 0.96) and 0.84 (95% CI: 0.82,
                                0.87), respectively. For the other abnormalities, the AUC ranged from 0.75 (95% CI:
                                0.70, 0.80) to 0.94 (95% CI: 0.91, 0.96). The controls had a high prevalence of other
                                lung diseases which can cause radiological manifestations similar to PTB (e.g., 26% had
                                pneumonia, 15% had lung malignancy, etc.). In a tertiary hospital in India, qXR
                                demonstrated moderate sensitivity and specificity for the detection of PTB. There is
                                likely a larger role for CAD software as a triage test for PTB at the primary care level
                                in settings where access to radiologists in limited. Larger prospective studies that can
                                better assess heterogeneity in important subgroups are needed.</p>

                            <a target="_blank" class="read-more"
                               href="https://www.nature.com/articles/s41598-019-56589-3"> Read full paper <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-24" class="post-title">Using artificial intelligence to read chest
                            radiographs for tuberculosis detection - A multi-site evaluation of the diagnostic accuracy
                            of three deep learning systems</h3>
                        <p class="post-authors">

                            Zhi Zhen Qin<sup>1</sup> ,

                            Melissa S. Sander<sup>2</sup> ,

                            Bishwa Rai<sup>3</sup> ,

                            Collins N. Titahong<sup>2</sup> ,

                            Santat Sudrungrot<sup>3</sup> ,

                            Sylvain N. Laah<sup>2, 4</sup> ,

                            Lal Mani Adhikari<sup>3</sup> ,

                            E. Jane Carter<sup>5</sup> ,

                            Lekha Puri<sup>1</sup> ,

                            Andrew J. Codlin<sup>1</sup> ,

                            Jacob Creswell<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Stop TB Partnership, Chemin du Pommier 40, 1218 Le Grand-Saconnex, Geneva,
                            Switzerland;&nbsp

                            2. Tuberculosis Reference Laboratory Bamenda, PO Box 586, Bamenda, Cameroon;&nbsp

                            3. International Organization for Migration, Migration Health Department, Kathmandu,
                            Nepal;&nbsp

                            4. Bamenda Regional Hospital, PO Box 818, Bamenda, Cameroon;&nbsp

                            5. Department of Medicine, Division of Pulmonary, Critical Care and Sleep, Warren Alpert
                            Medical School, Brown University, Rhode Island, US


                        </p>
                        <a href="/publications/2019/10/18/artificial-intelligence-chest-radiographs.html">
                            <p class="published-date">
                                Published:
                                18 October 2019, Nature Scientific Reports
                            </p>
                        </a>
                        <input type="checkbox" id="post-24">
                        <label class="show-abstract" for="post-24"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <p>Deep learning (DL) neural networks have only recently been employed to interpret chest
                                radiography (CXR) to screen and triage people for pulmonary tuberculosis (TB). No
                                published studies have compared multiple DL systems and populations. We conducted a
                                retrospective evaluation of three DL systems (CAD4TB, Lunit INSIGHT, and qXR) for
                                detecting TB-associated abnormalities in chest radiographs from outpatients in Nepal and
                                Cameroon. All 1196 individuals received a Xpert MTB/RIF assay and a CXR read by two
                                groups of radiologists and the DL systems. Xpert was used as the reference standard. The
                                area under the curve of the three systems was similar: Lunit (0.94, 95% CI: 0.93–0.96),
                                qXR (0.94, 95% CI: 0.92–0.97) and CAD4TB (0.92, 95% CI: 0.90–0.95). When matching the
                                sensitivity of the radiologists, the specificities of the DL systems were significantly
                                higher except for one. Using DL systems to read CXRs could reduce the number of Xpert
                                MTB/RIF tests needed by 66% while maintaining sensitivity at 95% or better. Using a
                                universal cutoff score resulted different performance in each site, highlighting the
                                need to select scores based on the population screened. These DL systems should be
                                considered by TB programs where human resources are constrained, and automated
                                technology is available.</p>

                            <a target="_blank" class="read-more"
                               href="https://www.nature.com/articles/s41598-019-51503-3"> Read full paper <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-23" class="post-title">Automated classification of X-rays as normal/Abnormal
                            using a high sensitivity deep learning algorithm</h3>
                        <p class="post-authors">

                            Vasanthakumar Venugopal<sup>1</sup> ,

                            Manoj TLD<sup>2</sup> ,

                            Bhargava Reddy<sup>2</sup> ,

                            Ankit Modi<sup>2</sup> ,

                            Salil Gupta<sup>1</sup> ,

                            Pooja Rao<sup>2</sup> ,

                            Prashant Warier<sup>2</sup> ,

                            Harsh Mahajan<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Centre for Advanced Research in Imaging, Neuroscience and Genomics;&nbsp

                            2. Qure.ai, Mumbai


                        </p>
                        <a href="/publications/2019/03/03/normal-abnormal-high-sensitivity.html">
                            <p class="published-date">
                                Presented:
                                03 March 2019, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-23">
                        <label class="show-abstract" for="post-23"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Majority of chest X-rays (CXRs) performed globally are normal radiologists spend
                                significant time ruling out these scans. We present a Deep Learning (DL) model trained
                                for the specific use of classifying CXRs into normal and abnormal, potentially reducing
                                time and cost associated with reporting normal studies.</p>

                            <h4 id="methods">Methods</h4>
                            <p>A DL algorithm trained on 1,150,084 CXRs and their corresponding reports was developed. A
                                retrospectively acquired independent test set of 430 CXRs (285 abnormal, 145 normal) was
                                analyzed by the algorithm classifying each X-Ray as normal or abnormal. Ground truth for
                                the independent test set was established by a sub-specialist chest radiologist with 8
                                years’ experience by reviewing every Chest X-Ray image with reference to the existing
                                report. Algorithm output was compared against ground truth and summary statistics were
                                calculated.</p>

                            <h4 id="results">Results</h4>
                            <p>The algorithm correctly classified 376 (87.44%) CXRs with sensitivity of 97.19% (95% CI -
                                94.54% to 98.78%) and specificity of 68.28% (95% CI - 60.04% to 75.75%). There were 46
                                (10.70%) false positives and 8 (1.86%) false negatives (FNs). Out of 8 FNs, 3 were
                                designated as clinically insignificant (mild, inactive fibrosis) and 5 as significant
                                (rib fractures, pneumothorax).</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>High-sensitivity DL algorithms can potentially be deployed for primary read of CXRs
                                enabling radiologists to spend appropriate time on abnormal cases, saving time and
                                thereby cost of reporting CXRs, especially on non-emergency situations, More in-depth
                                prospective trials are required to ascertain the overall impact of such algorithms.</p>

                            <a target="_blank" class="read-more"
                               href="https://ecronline.myesr.org/ecr2019/index.php?p=recording&t=recorded&lecture=automated-classification-of-chest-x-rays-as-normal-abnormal-using-a-high-sensitivity-deep-learning-algorithm">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-22" class="post-title">Automated Detection and Localization of
                            Pneumocephalus in Head CT scan</h3>
                        <p class="post-authors">

                            Swetha Tanamala<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>1</sup> ,

                            M Biviji<sup>2</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT & MRI Center, Nagpur, India


                        </p>
                        <a href="/publications/2019/03/01/automated-detection-localization-pneumocephalus.html">
                            <p class="published-date">
                                Presented:
                                01 March 2019, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-22">
                        <label class="show-abstract" for="post-22"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Pneumocephalus, accumulation of air in intracranial space, can lead to midline shift and
                                compression of brain. In this work, we detail the development of deep learning
                                algorithms for automated detection and localization of pneumocephalus in head CT
                                scans.</p>

                            <h4 id="methods">Methods</h4>
                            <p>Firstly, to localize the intracranial space from a given head CT scan, a
                                skullÂ­-stripping algorithm was developed using a randomly sampled anonymized dataset of
                                78 head CT scans (1608 slices). We sampled another anonymized dataset containing 83 head
                                CT scans (3546 slices) having pneumocephalus and 310 normal head CT scans which were
                                randomly sampled to represent natural distribution. These 3546 slices (932 slices had
                                pneumocephalus) were annotated for pneumocephalus regions. Then UÂ­Net based deep neural
                                network algorithm was trained on these scans to accurately predict the pneumocephalus
                                region . The predicted pneumocephalus region is refined by removing the regions outside
                                the intracranial space identified by the skull stripping algorithm. The refined
                                pneumocephalus region is then used to extract features. Using these features, a random
                                forest was trained to classify the presence of pneumocephalus in a scan. Areas under
                                receiver operating characteristics curves (AUC) were used to evaluate the
                                algorithms.</p>

                            <h4 id="results">Results</h4>
                            <p>An independent dataset of 1891 head CT scans (40 scans had pneumocephalus) was used for
                                testing above algorithms. AUC for the scan level predictions was 0.89. Sensitivity and
                                Specificity of 0.80 and 0.83 respectively were observed.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>In this work, we showed the efficacy of deep learning algorithms in localizing and
                                classifying the pneumocephalus accurately in a head CT scan.</p>

                            <a target="_blank" class="read-more"
                               href="https://ecronline.myesr.org/ecr2019/index.php?p=recording&t=recorded&lecture=automated-detection-and-localisation-of-pneumocephalus-in-head-ct-scan">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-21" class="post-title">Validation of Deep Learning Algorithms for Detection
                            of Critical Findings in Head CT Scans</h3>
                        <p class="post-authors">

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Swetha Tanamala<sup>1</sup> ,

                            M Biviji<sup>2</sup> ,

                            Norbert Campeau<sup>3</sup> ,

                            Vasanthakumar Venugopal<sup>4</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT & MRI Center, Nagpur, India;&nbsp

                            3. Department of Radiology, Mayo Clinic, Rochester, MN;&nbsp

                            4. Centre for Advanced Research in Imaging, Neurosciences and Genomics, New Delhi


                        </p>
                        <a href="/publications/2019/02/28/validation-deep-learning-critical-findings.html">
                            <p class="published-date">
                                Presented:
                                28 February 2019, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-21">
                        <label class="show-abstract" for="post-21"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>To validate set of deep learning algorithms for automated detection of key findings from
                                nonÂ­contrast head-CT scans: intracranial hemorrhage and its subtypes, calvarial
                                fractures, midline shift and mass effect.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We retrospectively collected a dataset containing 313,318 head-CT scans of which random
                                subset(Qure25k dataset) was used to validate and rest to develop algorithms. Additional
                                dataset(CQ500 dataset) was collected from different centers to validate algorithms.
                                Patients with postÂ­operative defect or age&lt;7 were excluded from all datasets. Three
                                independent radiologists read each scan in CQ500 dataset. Original clinical radiology
                                report and consensus of readers were considered as gold standards for Qure25k and CQ500
                                datasets respectively. Areas under receiver operating characteristics curves(AUCs) were
                                used to evaluate algorithms.</p>

                            <h4 id="results">Results</h4>
                            <p>After exclusion, Qure25k dataset contained 21,095 scans(mean-age 43;43% female) while
                                CQ500 dataset consisted of 491(mean-age 48;36% female) scans. On Qure25k dataset,
                                algorithms achieved an AUC of 0.92 for detecting intracranial
                                hemorrhage(0.90-intraparenchymal, 0.96-intraventricular, 0.92-subdural, 0.93-extradural,
                                and 0.90-subarachnoid hemorrhages). On CQ500 dataset, AUC was 0.94 for intracranial
                                haemorrhage(0.95, 0.93, 0.95, 0.97, and 0.96 respectively). AUCs on Qure25k dataset were
                                0.92 for calvarial fractures, 0.93 for midline shift, and 0.86 for mass effect, while
                                AUCs on CQ500 dataset were 0.96, 0.97 and 0.92 respectively.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>This study demonstrates that deep learning algorithms can identify head-CT scan
                                abnormalities requiring urgent attention with high AUCs.</p>

                            <a target="_blank" class="read-more"
                               href="https://ecronline.myesr.org/ecr2019/index.php?p=recording&t=recorded&lecture=validation-of-deep-learning-algorithms-for-the-detection-of-critical-findings-in-head-ct-scans">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-20" class="post-title">Prospective evaluation of a deep learning algorithm
                            deployed in an urban imaging centre to notify clinicians of head CT scans with critical
                            abnormalities</h3>
                        <p class="post-authors">

                            Swetha Tanamala<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>1</sup> ,

                            M Biviji<sup>2</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Pooja Rao<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT & MRI Center, Nagpur, India


                        </p>
                        <a href="/publications/2019/02/28/prospective-deep-learning-urban-headct.html">
                            <p class="published-date">
                                Presented:
                                28 February 2019, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-20">
                        <label class="show-abstract" for="post-20"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Non-contrast Head-CT scans are primary imaging modality for evaluating patients with
                                trauma or stroke. While results of deep learning algorithms to identify head-CT scans
                                containing critical abnormalities has been published in retrospective studies, effects
                                of deployment of such an algorithm in a real-world setting, with mobile notifications to
                                clinician remain unstudied. In this prospective study, we evaluated performance of such
                                an automated triage system in an urban 24-hour imaging facility.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We developed an accurate deep neural network algorithm that identifies and localizes
                                intracranial bleeds, cranial fractures, mass effect and midline shift on non-contrast
                                head-CT scans. The algorithm is deployed in clinical imaging facility in conjunction
                                with an on-premise module that automatically selects eligible scans from PACS and
                                uploads them to cloud-based algorithm for processing. Once processed, cloud algorithm
                                returns an additional series, viewable as an overlay over the original, and a text
                                notification to radiologist with preview images. Mobile notifications facilitated
                                confirmation of the detected abnormalities. We studied the performance of the automated
                                system over 60 days.</p>

                            <h4 id="results">Results</h4>
                            <p>748 CT scans were taken over 60 days, of which 194 were non-contrast head-CT scans and
                                these are evaluated by senior radiologist. Sensitivity, specificity, AUC and average
                                time to notification of head-CT scans with critical abnormalities were Â 0.90 (95% CI
                                0.74-0.98), 0.86 (0.80-0.91), 0.97 (0.92-1.00) and 3.2 minutes respectively.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>An automated triage system in a radiology facility results in rapid notification of
                                critical scans, with a low false positive rate and this may be used to expedite
                                treatment initiation.</p>

                            <a target="_blank" class="read-more"
                               href="https://ecronline.myesr.org/ecr2019/index.php?p=recording&t=recorded&lecture=prospective-evaluation-of-a-deep-learning-algorithm-deployed-in-an-urban-imaging-centre-to-notify-clinicians-of-head-ct-scans-with-critical-abnormalities">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-19" class="post-title">Deep Learning for Infarct Detection and Localization
                            from Head CT Scans</h3>
                        <p class="post-authors">

                            Rohit Ghosh<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Swetha Tanamala<sup>1</sup> ,

                            Norbert Campeau<sup>2</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. Department of Radiology, Mayo Clinic, Rochester, MN


                        </p>
                        <a href="/publications/2019/02/28/deep-learning-infarct-localization-detection.html">
                            <p class="published-date">
                                Presented:
                                28 February 2019, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-19">
                        <label class="show-abstract" for="post-19"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>The purpose of this study was to use a deep learning algorithm to detect and localize
                                subacute and chronic ischemic infarcts on head CT scans for use in automated volumetric
                                progression tracking.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We sampled 308 head CT scans (11840 slices) which were reported with chronic or subacute
                                infarct. The infarcted regions in 11840 infarct-positive slices were marked. We trained
                                segmentation algorithm to predict a heatmap of infarct lesion. The heatmap was used to
                                derive scan level features representative of lesion density and volume to train a random
                                forest to predict scan-level probabilities of chronic infarct. Area under receiver
                                operating characteristics curves (AUC) were used to evaluate scan level predictions.</p>

                            <h4 id="results">Results</h4>
                            <p>The algorithm was validated on an independent dataset of 1610 head CT scans containing 78
                                chronic &amp; 9 subacute infarct, 45 chronic ICH, 6 glioblastomas. The distribution of
                                infarct affected territories was - 52.9% MCA, 33.3 % PCA, 9.3% ACA and 4.7%
                                vertebrobasilar territories. The algorithm yielded AUC of 0.8474 (95% CI 0.7964 -
                                0.8984) for scan level predictions. It identified 8 of 9 subacute infarcts (88.89%
                                recall) and 70 out of 78 chronic infarcts (89.74% recall). The eight missed chronic
                                infarcts constituted of 3 lacunar and 2 hemorrhagic. The volumes of predicted infarct
                                lesions ranged from 1 mL - 526 mL with mean prediction volume as 55.60mL.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>The study demonstrates the capability of deep learning algorithms to accurately
                                differentiate infarcts from infarct mimics.</p>

                            <a target="_blank" class="read-more"
                               href="https://ecronline.myesr.org/ecr2019/index.php?p=recording&t=recorded&lecture=deep-learning-for-infarct-detection-and-localisation-from-head-ct-scans">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-18" class="post-title">Automated Detection of Midline Shift and Mass Effect
                            from Head CT Scans using Deep Learning</h3>
                        <p class="post-authors">

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Swetha Tanamala<sup>1</sup> ,

                            M Biviji<sup>2</sup> ,

                            Norbert Campeau<sup>3</sup> ,

                            Vasanthakumar Venugopal<sup>4</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT & MRI Center, Nagpur, India;&nbsp

                            3. Department of Radiology, Mayo Clinic, Rochester, MN;&nbsp

                            4. Centre for Advanced Research in Imaging, Neurosciences and Genomics, New Delhi


                        </p>
                        <a href="/publications/2019/02/28/automated-detection-mass-effect-midline-shift.html">
                            <p class="published-date">
                                Presented:
                                28 February 2019, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-18">
                        <label class="show-abstract" for="post-18"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Mass effect and Midline shift are most critical and timeÂ­ sensitive abnormalities that
                                can be readily detected on head-CT scan. We describe development and validation of deep
                                learning algorithms to automatically detect the mentioned abnormalities.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We labeled slices from 699 anonymized nonÂ­contrast head-CT scans for the presence or
                                absence of mass effect and midline shift in that slice. Number of scans(slices) with
                                mass effect were 320(3143) and midline shift were 249(2074). We used these labels to
                                train a modified ResNet18, a popular convolutional neural network to predict softmax
                                based confidences for the presence of mass effect and midline shift in a slice. We
                                modified the network by using two parallel fully connected(FC) layers in place of a
                                single FC layer. The confidences at the slice­-level were combined using random forest
                                to predict the scan-level confidence for the presence of mass effect and midline shift.
                                A separate dataset(CQ500 dataset) was collected for the validation of the algorithm.
                                Three senior radiologists independently read each scan in this dataset. Consensus of the
                                readers’ opinion was used as the gold standard. We used areas under receiver operating
                                characteristics curves(AUC) to evaluate the algorithm.</p>

                            <h4 id="results">Results</h4>
                            <p>CQ500 dataset contained 491 scans of which number of scans with mass effect and midline
                                shift were 99 and 47 respectively. AUC for detecting mass effect was 0.92(95%CI
                                0.89-0.95) and for detecting midline shift was 0.97(95%CI 0.94-0.99).</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>We show that a deep learning algorithm can be trained to accurately detect mass effect
                                and midline shift from head CT scans.</p>

                            <a target="_blank" class="read-more"
                               href="https://ecronline.myesr.org/ecr2019/index.php?p=recording&t=recorded&lecture=automated-detection-of-midline-shift-and-mass-effect-from-head-ct-scans-using-deep-learning">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-17" class="post-title">Deep learning algorithms for detection of critical
                            findings in head CT scans - A retrospective study</h3>
                        <p class="post-authors">

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Swetha Tanamala<sup>1</sup> ,

                            Mustafa Biviji<sup>2</sup> ,

                            Norbert G Campeau<sup>3</sup> ,

                            Vasantha Kumar Venugopal<sup>4</sup> ,

                            Vidur Mahajan<sup>4</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Prashant Warier<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai, India;&nbsp

                            2. CT & MRI Center, Dhantoli, Nagpur, India;&nbsp

                            3. Department of Radiology, Mayo Clinic, Rochester, MN, USA;&nbsp

                            4. Centre for Advanced Research in Imaging, Neurosciences and Genomics, New Delhi, India


                        </p>
                        <a href="/publications/2018/10/11/head-ct-scans-critical-findings.html">
                            <p class="published-date">
                                Published:
                                11 October 2018, The Lancet
                            </p>
                        </a>
                        <input type="checkbox" id="post-17">
                        <label class="show-abstract" for="post-17"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background">Background</h4>

                            <p>Non-contrast head CT scan is the current standard for initial imaging of patients with
                                head trauma or stroke symptoms. We aimed to develop and validate a set of deep learning
                                algorithms for automated detection of the following key findings from these scans:
                                intracranial haemorrhage and its types (ie, intraparenchymal, intraventricular,
                                subdural, extradural, and subarachnoid); calvarial fractures; midline shift; and mass
                                effect.</p>

                            <h4 id="methods">Methods</h4>

                            <p>We retrospectively collected a dataset containing 313 318 head CT scans together with
                                their clinical reports from around 20 centres in India between Jan 1, 2011, and June 1,
                                2017. A randomly selected part of this dataset (Qure25k dataset) was used for validation
                                and the rest was used to develop algorithms. An additional validation dataset (CQ500
                                dataset) was collected in two batches from centres that were di erent from those used
                                for the development and Qure25k datasets. We excluded postoperative scans and scans of
                                patients younger than 7 years. The original clinical radiology report and consensus of
                                three independent radiologists were considered as gold standard for the Qure25k and
                                CQ500 datasets, respectively. Areas under the receiver operating characteristic curves
                                (AUCs) were primarily used to assess the algorithms.</p>

                            <h4 id="findings">Findings</h4>

                            <p>The Qure25k dataset contained 21 095 scans (mean age 43 years; 9030 [43%] female
                                patients), and the CQ500 dataset consisted of 214 scans in the rst batch (mean age 43
                                years; 94 [44%] female patients) and 277 scans in the second batch (mean age 52 years;
                                84 [30%] female patients). On the Qure25k dataset, the algorithms achieved an AUC of
                                0·92 (95% CI 0·91–0·93) for detecting intracranial haemorrhage (0·90 [0·89–0·91] for
                                intraparenchymal, 0·96 [0·94–0·97] for intraventricular, 0·92 [0·90–0·93] for subdural,
                                0·93 [0·91–0·95] for extradural, and 0·90 [0·89–0·92] for subarachnoid). On the CQ500
                                dataset, AUC was 0·94 (0·92–0·97) for intracranial haemorrhage (0·95 [0·93–0·98], 0·93
                                [0·87–1·00], 0·95 [0·91–0·99], 0·97 [0·91–1·00], and 0·96 [0·92–0·99], respectively).
                                AUCs on the Qure25k dataset were 0·92 (0·91–0·94) for calvarial fractures, 0·93
                                (0·91–0·94) for midline shift, and 0·86 (0·85–0·87) for mass e ect, while AUCs on the
                                CQ500 dataset were 0·96 (0·92–1·00), 0·97 (0·94–1·00), and 0·92 (0·89–0·95),
                                respectively.</p>

                            <h4 id="interpretation">Interpretation</h4>

                            <p>Our results show that deep learning algorithms can accurately identify head CT scan
                                abnormalities requiring urgent attention, opening up the possibility to use these
                                algorithms to automate the triage process.</p>

                            <a target="_blank" class="read-more"
                               href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(18)31645-3/fulltext">
                                Read full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-16" class="post-title">Deep learning in chest radiography: Detection of
                            findings and presence of change</h3>
                        <p class="post-authors">

                            Ramandeep Singh<sup>1</sup> ,

                            Mannudeep K. Kalra<sup>1</sup> ,

                            Chayanin Nitiwarangkul<sup>1,2</sup> ,

                            John A. Patti<sup>1</sup> ,

                            Fatemeh Homayounieh<sup>1</sup> ,

                            Atul Padole<sup>1</sup> ,

                            Pooja Rao<sup>3</sup> ,

                            Preetham Putha<sup>3</sup> ,

                            Victorine V. Muse<sup>1</sup> ,

                            Amita Sharma<sup>1</sup> ,

                            Subba R. Digumarthy<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Department of Radiology, Massachusetts General Hospital, Boston, Massachusetts, United
                            States of America, Harvard Medical School, Boston, Massachusetts, United States of
                            America;&nbsp

                            2. Division of Diagnostic Radiology, Department of Diagnostic and Therapeutic Radiology,
                            Faculty of Medicine, Ramathibodi Hospital, Mahidol University, Bangkok, Thailand;&nbsp

                            3. Qure.ai, 101 Raheja Titanium, Goregaon East, Mumbai, India


                        </p>
                        <a href="/publications/2018/10/04/plos-one-chest-radiography.html">
                            <p class="published-date">
                                Published:
                                04 October 2018, PLOS One
                            </p>
                        </a>
                        <input type="checkbox" id="post-16">
                        <label class="show-abstract" for="post-16"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background">Background</h4>

                            <p>Deep learning (DL) based solutions have been proposed for interpretation of several
                                imaging modalities including radiography, CT, and MR. For chest radiographs, DL
                                algorithms have found success in the evaluation of abnormalities such as lung nodules,
                                pulmonary tuberculosis, cystic fibrosis, pneumoconiosis, and location of peripherally
                                inserted central catheters. Chest radiography represents the most commonly performed
                                radiological test for a multitude of non-emergent and emergent clinical indications.
                                This study aims to assess accuracy of deep learning (DL) algorithm for detection of
                                abnormalities on routine frontal chest radiographs (CXR), and assessment of stability or
                                change in findings over serial radiographs.</p>

                            <h4 id="methods-and-findings">Methods and Findings</h4>

                            <p>We processed 874 de-identified frontal CXR from 724 adult patients (&gt; 18 years) with
                                DL (Qure AI). Scores and prediction statistics from DL were generated and recorded for
                                the presence of pulmonary opacities, pleural effusions, hilar prominence, and enlarged
                                cardiac silhouette. To establish a standard of reference (SOR), two thoracic
                                radiologists assessed all CXR for these abnormalities. Four other radiologists (test
                                radiologists), unaware of SOR and DL findings, independently assessed the presence of
                                radiographic abnormalities. A total 724 radiographs were assessed for detection of
                                findings. A subset of 150 radiographs with follow up examinations was used to asses
                                change over time. Data were analyzed with receiver operating characteristics analyses
                                and post-hoc power analysis.</p>

                            <h4 id="results">Results</h4>

                            <p>About 42% (305/ 724) CXR had no findings according to SOR; single and multiple
                                abnormalities were seen in 23% (168/724) and 35% (251/724) of CXR. There was no
                                statistical difference between DL and SOR for all abnormalities (p = 0.2–0.8). The area
                                under the curve (AUC) for DL and test radiologists ranged between 0.837–0.929 and
                                0.693–0.923, respectively. DL had lowest AUC (0.758) for assessing changes in pulmonary
                                opacities over follow up CXR. Presence of chest wall implanted devices negatively
                                affected the accuracy of DL algorithm for evaluation of pulmonary and hilar
                                abnormalities.</p>

                            <h4 id="conclusions">Conclusions</h4>

                            <p>DL algorithm can aid in interpretation of CXR findings and their stability over follow up
                                CXR. However, in its present version, it is unlikely to replace radiologists due to its
                                limited specificity for categorizing specific findings.</p>


                            <a target="_blank" class="read-more"
                               href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0204155"> Read
                                full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-15" class="post-title">Can Artificial Intelligence Reliably Report Chest
                            X-Rays? Radiologist Validation of an Algorithm trained on 1.2 Million X-Rays</h3>
                        <p class="post-authors">

                            Preetham Putha<sup>1</sup> ,

                            Manoj Tadepalli<sup>1</sup> ,

                            Bhargava Reddy<sup>1</sup> ,

                            Tarun Raj<sup>1</sup> ,

                            Justy Antony Chiramal<sup>1</sup> ,

                            Shalini Govil<sup>2</sup> ,

                            Namita Sinha<sup>2</sup> ,

                            Manjunath KS<sup>2</sup> ,

                            Sundeep Reddivari<sup>2</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Prashant Warier<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai, India;&nbsp

                            2. Columbia Asia Radiology Group, Bengaluru, India


                        </p>
                        <a href="/publications/2018/07/19/radiologist-validation-chest-Xrays.html">
                            <p class="published-date">
                                Published:
                                19 July 2018
                            </p>
                        </a>
                        <input type="checkbox" id="post-15">
                        <label class="show-abstract" for="post-15"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background-and-objectives">Background and Objectives</h4>

                            <p>Chest x-rays are the most commonly performed, cost-effective diagnostic imaging tests
                                ordered by physicians. A clinically validated, automated artificial intelligence system
                                that can reliably separate normal from abnormal would be invaluable in addressing the
                                problem of reporting backlogs and the lack of radiologists in low-resource settings. The
                                aim of this study was to develop and validate a deep learning system to detect chest
                                x-ray abnormalities.</p>

                            <h4 id="methods">Methods</h4>

                            <p>A deep learning system was trained on 1.2 million x-rays and their corresponding
                                radiology reports to identify abnormal x-rays and the following specific abnormalities:
                                blunted costophrenic angle, calcification, cardiomegaly, cavity, consolidation,
                                fibrosis, hilar enlargement, opacity and pleural effusion. The system was tested versus
                                a 3-radiologist majority on an independent, retrospectively collected de-identified set
                                of 2000 x-rays. The primary accuracy measure was area under the ROC curve (AUC),
                                estimated separately for each abnormality as well as for normal versus abnormal
                                reports.</p>

                            <h4 id="results">Results</h4>
                            <p>The deep learning system demonstrated an AUC of 0.93(CI 0.92-0.94) for detection of
                                abnormal scans, and AUC(CI) of 0.94(0.92-0.97),0.88(0.85-0.91), 0.97(0.95-0.99),
                                0.92(0.82-1), 0.94(0.91-0.97), 0.92(0.88-0.95), 0.89(0.84-0.94), 0.93(0.92-0.95),
                                0.98(0.97-1), 0.93(0.0.87-0.99) for the detection of blunted CP angle, calcification,
                                cardiomegaly, cavity, consolidation, fibrosis,hilar enlargement, opacity and pleural
                                effusion respectively.</p>

                            <h4 id="conclusions">Conclusions</h4>

                            <p>Our study shows that a deep learning algorithm trained on a large quantity of labelled
                                data can accurately detect abnormalities on chest x-rays. As these systems further
                                increase in accuracy, the feasibility of using artificial intelligence to extend the
                                reach of chest x-ray interpretation and improve reporting efficiency will increase in
                                tandem.</p>

                            <a target="_blank" class="read-more" href="https://arxiv.org/abs/1807.07455"> Read full
                                paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-14" class="post-title">Machine Learning Methods Improve Prognostication,
                            Identify Clinically Distinct Phenotypes, and Detect Heterogeneity in Response to Therapy in
                            a Large Cohort of Heart Failure Patients</h3>
                        <p class="post-authors">

                            Tariq Ahmad<sup>1</sup> ,

                            Lars H. Lund<sup>2</sup> ,

                            Pooja Rao<sup>3</sup> ,

                            Rohit Ghosh<sup>3</sup> ,

                            Prashant Warier<sup>3</sup> ,

                            Benjamin Vaccaro<sup>1</sup> ,

                            Ulf Dahlström<sup>4</sup> ,

                            Christopher M. O'Connor<sup>5</sup> ,

                            G. Michael Felker<sup>5</sup> ,

                            Nihar R. Desai<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Section of Cardiovascular Medicine and Center for Outcomes Research, Yale University
                            School of Medicine New Haven, CT;&nbsp

                            2. Department of Cardiology, Karolinska Institutet Department of Medicine and Karolinska
                            University Hospital, Stockholm, Sweden;&nbsp

                            3. Qure.ai, Mumbai, India;&nbsp

                            4. Department of Medicine and Health Sciences, Linköping University, Linköping, Sweden;&nbsp

                            5. Duke Clinical Research Institute, Duke University, Durham, NC


                        </p>
                        <a href="/publications/2018/04/12/heart-failure.html">
                            <p class="published-date">
                                Published:
                                12 April 2018, Journal of the American Heart Association
                            </p>
                        </a>
                        <input type="checkbox" id="post-14">
                        <label class="show-abstract" for="post-14"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background">Background</h4>

                            <p>Whereas heart failure (HF) is a complex clinical syndrome, conventional approaches to its
                                management have treated it as a singular disease, leading to inadequate patient care and
                                inefficient clinical trials. We hypothesized that applying advanced analytics to a large
                                cohort of HF patients would improve prognostication of outcomes, identify distinct
                                patient phenotypes, and detect heterogeneity in treatment response.</p>

                            <h4 id="methods-and-results">Methods and Results</h4>

                            <p>The Swedish Heart Failure Registry is a nationwide registry collecting detailed
                                demographic, clinical, laboratory, and medication data and linked to databases with
                                outcome information. We applied random forest modeling to identify predictors of 1‐year
                                survival. Cluster analysis was performed and validated using serial bootstrapping.
                                Association between clusters and survival was assessed with Cox proportional hazards
                                modeling and interaction testing was performed to assess for heterogeneity in response
                                to HF pharmacotherapy across propensity‐matched clusters. Our study included 44 886 HF
                                patients enrolled in the Swedish Heart Failure Registry between 2000 and 2012. Random
                                forest modeling demonstrated excellent calibration and discrimination for survival
                                (C‐statistic=0.83) whereas left ventricular ejection fraction did not
                                (C‐statistic=0.52): there were no meaningful differences per strata of left ventricular
                                ejection fraction (1‐year survival: 80%, 81%, 83%, and 84%). Cluster analysis using the
                                8 highest predictive variables identified 4 clinically relevant subgroups of HF with
                                marked differences in 1‐year survival. There were significant interactions between
                                propensity‐matched clusters (across age, sex, and left ventricular ejection fraction and
                                the following medications: diuretics, angiotensin‐converting enzyme inhibitors,
                                β‐blockers, and nitrates, P &lt; 0.001, all).</p>

                            <h4 id="conclusions">Conclusions</h4>

                            <p>Machine learning algorithms accurately predicted outcomes in a large data set of HF
                                patients. Cluster analysis identified 4 distinct phenotypes that differed significantly
                                in outcomes and in response to therapeutics. Use of these novel analytic approaches has
                                the potential to enhance effectiveness of current therapies and transform future HF
                                clinical trials.</p>

                            <a target="_blank" class="read-more" href="http://jaha.ahajournals.org/content/7/8/e008081">
                                Read full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-13" class="post-title">Development and Validation of Deep Learning
                            Algorithms for Detection of Critical Findings in Head CT Scans</h3>
                        <p class="post-authors">

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Swetha Tanamala<sup>1</sup> ,

                            Mustafa Biviji<sup>2</sup> ,

                            Norbert G. Campeau<sup>3</sup> ,

                            Vasantha Kumar Venugopal<sup>4</sup> ,

                            Vidur Mahajan<sup>4</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Prashant Warier<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT and MRI center, Nagpur;&nbsp

                            3. Department of Radiology, Mayo Clinic, Rochester, MN;&nbsp

                            4. Centre for Advanced Research in Imaging, Neurosciences and Genomics, New Delhi


                        </p>
                        <a href="/publications/2018/03/13/headct-study.html">
                            <p class="published-date">
                                Published:
                                13 March 2018
                            </p>
                        </a>
                        <input type="checkbox" id="post-13">
                        <label class="show-abstract" for="post-13"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="importance">Importance</h4>
                            <p>Non-contrast head CT scan is the current standard for initial imaging of patients with
                                head trauma or stroke symptoms.</p>

                            <h4 id="objective">Objective</h4>
                            <p>To develop and validate a set of deep learning algorithms for automated detection of
                                following key findings from non-contrast head CT scans: intracranial hemorrhage (ICH)
                                and its types, intraparenchymal (IPH), intraventricular (IVH), subdural (SDH), ex-
                                tradural (EDH) and subarachnoid (SAH) hemorrhages, calvarial fractures, midline shift
                                and mass effect.</p>

                            <h4 id="design-and-settings">Design And Settings</h4>
                            <p>We retrospectively collected a dataset containing 313,318 head CT scans along with their
                                clinical reports from various centers. A part of this dataset (Qure25k dataset) was used
                                to validate and the rest to develop algorithms. Additionally, a dataset (CQ500 dataset)
                                was collected from different centers in two batches B1 &amp; B2 to clinically validate
                                the algorithms.</p>

                            <h4 id="main-outcomes-and-measures">Main Outcomes And Measures</h4>
                            <p>Original clinical radiology report and consensus of three independent radiologists were
                                considered as gold standard for Qure25k and CQ500 datasets respectively. Area under
                                receiver operating characteristics curve (AUC) for each finding was primarily used to
                                evaluate the algorithms.</p>

                            <h4 id="results">Results</h4>
                            <p>Qure25k dataset contained 21,095 scans (mean age 43.31; 42.87% female) while batches B1
                                and B2 of CQ500 dataset consisted of 214 (mean age 43.40; 43.92% female) and 277 (mean
                                age 51.70; 30.31% female) scans respectively. On Qure25k dataset, the algorithms
                                achieved AUCs of 0.9194, 0.8977, 0.9559, 0.9161, 0.9288 and 0.9044 for detecting ICH,
                                IPH, IVH, SDH, EDH and SAH respectively. AUCs for the same on CQ500 dataset were 0.9419,
                                0.9544, 0.9310, 0.9521, 0.9731 and 0.9574 respectively. For detecting calvarial
                                fractures, midline shift and mass effect, AUCs on Qure25k dataset were 0.9244, 0.9276
                                and 0.8583 respectively, while AUCs on CQ500 dataset were 0.9624, 0.9697 and 0.9216
                                respectively.</p>

                            <h4 id="conclusions-and-relevance">Conclusions And Relevance</h4>
                            <p>This study demonstrates that deep learning algorithms can accurately identify head CT
                                scan abnormalities requiring urgent attention. This opens up the possibility to use
                                these algorithms to automate the triage process. They may also provide a lower bound for
                                quality and consistency of radiological interpretation.</p>

                            <a target="_blank" class="read-more" href="http://arxiv.org/abs/1803.05854"> Read full paper
                                <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-12" class="post-title">Efficacy of deep learning for screening pulmonary
                            tuberculosis</h3>
                        <p class="post-authors">

                            Preetham Putha<sup>1</sup> ,

                            Manoj TLD<sup>1</sup> ,

                            Shubham Jain<sup>1</sup> ,

                            Justy Antony Chiramal<sup>1</sup> ,

                            Tarun R Nimmada<sup>1</sup> ,

                            Prashant Warier<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai


                        </p>
                        <a href="/publications/2018/03/04/tb-ecr.html">
                            <p class="published-date">
                                Presented:
                                04 March 2018, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-12">
                        <label class="show-abstract" for="post-12"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Chest X-rays(CXR), being highly sensitive, serve as a Screening tool in TB diagnosis.
                                Though there are no classical features diagnostic of TB on CXR, there are a few patterns
                                that can be used as supportive evidence. In Resource limited settings, developing Deep
                                Learning algorithms for CXR based TB screening, could reduce diagnostic delay. Our
                                algorithm screens for 8 abnormal patterns(TB tags)- Pleural effusion, blunted CP,
                                Atelectasis, Fibrosis, Opacity, Nodules, Calcification and Cavity. It reports ‘No
                                Abnormality Detected’ if none of these patterns are present on CXR.</p>

                            <h4 id="methods">Methods</h4>
                            <p>An anonymized dataset of 423,218 CXRs with matched radiologist reports across (22 models,
                                9 manufacturers, 166 centres in India) was used to generate training data for the deep
                                learning models. Natural Language Processing techniques were used to extract TB tags
                                from these reports. Deep learning systems were trained to predict the probability of the
                                presence/absence of each TB tag along with heat-maps that highlight abnormal regions in
                                the CXR for each positive result.</p>

                            <h4 id="results">Results</h4>
                            <p>We validated the screening algorithm on 3 datasets external to our training set- two
                                public datasets maintained by NIH(from Montgomery and Shenzen) and a third from NIRT,
                                India. The Area under the Receiver Operating Curve (AUC-ROC) for TB prediction was 0.91,
                                0.87 and 0.83 respectively.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>Training on a diversified dataset enabled good performance on samples from completely
                                different demographics. After further validation of it’s robustness against variation,
                                the system can be deployed at scale to improve the current systems for TB screening
                                significantly</p>

                            <a target="_blank" class="read-more"
                               href="http://ecronline.myesr.org/ecr2018/?p=recording&t=recorded&lecture=efficacy-of-deep-learning-for-screening-pulmonary-tuberculosis">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-11" class="post-title">Automated detection of intra- and extra-axial
                            haemorrhages on CT brain images using deep neural networks</h3>
                        <p class="post-authors">

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Mustafa Biviji<sup>2</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Prashant Warier<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT and MRI center, Nagpur


                        </p>
                        <a href="/publications/2018/03/04/hemorrhages-ecr.html">
                            <p class="published-date">
                                Presented:
                                04 March 2018, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-11">
                        <label class="show-abstract" for="post-11"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>To develop and validate a deep neural network-based algorithm for automated, rapid and
                                accurate detection from head CT for the following haemorrhages: intracerebral (ICH),
                                subdural (SDH), extradural (EDH) and subarachnoid (SAH).</p>

                            <h4 id="methods">Methods</h4>
                            <p>An anonymised database of head CTs was searched for non-contrast scans which were
                                reported with any of ICH, SDH, EDH, SAH and those which were reported with neither of
                                these. Each slice of these scans is manually tagged with the haemorrhages that are
                                visible in that slice. In all, 3040 scans (116227 slices) were annotated, of which
                                number of scans(slices) with ICH, SDH, EDH, SAH and neither of these are 781(6957),
                                493(6593), 742(6880), 561(5609) and 944(92999), respectively. Our deep learning model is
                                a modified ResNet18 with 4 parallel final fully connected layers for each of the
                                haemorrhages. This model is trained on the slices from the annotated dataset to make
                                slice-level decisions. Random forests are trained with ResNet’s softmax outputs for all
                                the slices in a scan as features to make scan-level decisions.</p>

                            <h4 id="results">Results</h4>
                            <p>A different set of 2993 scans, uniformly sampled from the database without any exclusion
                                criterion, is used for testing the scan-level decisions. Number of scans with ICH, SDH,
                                EDH and SAH in this set are 123, 58, 41 and 62, respectively. Area under the receiver
                                operating curve (AUC) for scan-level decisions for ICH, SDH, EDH and SAH are 0.91, 0.90,
                                0.90 and 0.90, respectively. Algorithm takes less than 1s to produce the decision for a
                                scan.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>Deep learning can accurately detect intra- and extra-axial haemorrhages from head
                                CTs.</p>

                            <a target="_blank" class="read-more"
                               href="http://ecronline.myesr.org/ecr2018/index.php?p=recording&t=recorded&lecture=automated-detection-of-intra-and-extra-axial-haemorrhages-on-ct-brain-images-using-deep-neural-networks">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-10" class="post-title">Identifying pulmonary consolidation in Chest X Rays
                            using deep learning</h3>
                        <p class="post-authors">

                            Tarun R Nimmada<sup>1</sup> ,

                            Preetham Putha<sup>1</sup> ,

                            Manoj TLD<sup>1</sup> ,

                            Shubham Jain<sup>1</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Prashant Warier<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai


                        </p>
                        <a href="/publications/2018/03/04/consolidation-ecr.html">
                            <p class="published-date">
                                Presented:
                                04 March 2018, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-10">
                        <label class="show-abstract" for="post-10"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Chest x-rays are widely used to identify pulmonary consolidation because they are highly
                                accessible, cheap and sensitive. Automating the diagnosis in chest x-rays can reduce
                                diagnostic delay, especially in resource-limited settings.</p>

                            <h4 id="methods">Methods</h4>
                            <p>Anonymised dataset of 423,218 chest x-rays with corresponding reports (collected from 166
                                centres across India spanning 22 x-ray machine variants from 9 manufacturers) is used
                                for training and validation. x-rays with consolidation are identified from their reports
                                using natural language processing techniques. Images are preprocessed to a standard size
                                and normalised to remove source dependency. These images are trained using deep residual
                                neural networks. Multiple models are trained on various selective subsets of the dataset
                                along with one model trained on entire data set. Scores yielded by each of these models
                                is passed through a 2-layer neural network to generate final probabilities for presence
                                of consolidation in an x-ray.</p>

                            <h4 id="results">Results</h4>
                            <p>The model is validated and tested on a test dataset that is uniformly sampled from the
                                parent dataset without any exclusion criteria. Sensitivity and specificity for the tag
                                has been observed as 0.81 and 0.80, respectively. Area under the Receiver Operating
                                Curve (AUC-ROC) was observed as 0.88.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>Deep learning can be used to diagnose pulmonary consolidation in chest x-rays with models
                                trained on a generalised dataset with samples from multiple demographics. This model
                                performs better than a model trained on controlled dataset and is suited for a real
                                world setting where x-ray quality may not be consistent.</p>


                            <a target="_blank" class="read-more"
                               href="http://ecronline.myesr.org/ecr2018/index.php?p=recording&t=recorded&lecture=identifying-pulmonary-consolidation-in-chest-x-rays-using-deep-learning">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-9" class="post-title">Automatic detection of generalised cerebral atrophy
                            using deep neural networks from head CT scans</h3>
                        <p class="post-authors">

                            Swetha Tanamala<sup>1</sup> ,

                            Rohit Ghosh<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Mustafa Biviji<sup>2</sup> ,

                            Pooja Rao<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT and MRI center, Nagpur


                        </p>
                        <a href="/publications/2018/03/04/atrophy-ecr.html">
                            <p class="published-date">
                                Presented:
                                04 March 2018, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-9">
                        <label class="show-abstract" for="post-9"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Features of generalised cerebral atrophy on brain CT images are the marker of
                                neurodegenerative diseases of the brain. Our study aims at automated diagnosis of
                                generalised cerebral atrophy on brain CT images using deep neural networks thereby
                                offering an objective early diagnosis.</p>

                            <h4 id="methods">Methods</h4>
                            <p>An anonymised dataset containing 78 head CT scans (1608 slices) was used to train and
                                validate a skull-stripping algorithm. The intracranial region was marked out slice by
                                slice in each scan. Then a U-Net-based deep neural network was trained on these
                                annotations to strip the skull from each slice. A second anonymised dataset containing
                                2189 CT scans (231 scans with atrophy) was used to train and validate an atrophy
                                detection algorithm. First, an image registration technique was applied on the predicted
                                intracranial region to align all scans to a standard head CT scan. The parenchymal and
                                CSF volume was calculated by thresholding Hounsfield units from the intracranial region.
                                The ratio of CSF volume to parenchymal volume from each slice of the aligned CT scan and
                                the age of the patient were used as features to train a random forest algorithm that
                                decides if the scan shows generalised cerebral atrophy.</p>

                            <h4 id="results">Results</h4>
                            <p>An independent set of 3000 head CT scans (347 scans with atrophy) was used to test the
                                algorithm. Area under the receiver operating curve (AUC) for scan-level decisions is
                                0.86. Predictions on each patient takes time &lt; 45s.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>Deep convolutional networks can accurately detect generalised cerebral atrophy given a CT
                                scan.</p>


                            <a target="_blank" class="read-more"
                               href="http://ecronline.myesr.org/ecr2018/index.php?p=recording&t=recorded&lecture=automatic-detection-of-generalised-cerebral-atrophy-using-deep-neural-networks-from-head-ct-scans">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-8" class="post-title">Clinical validation of a deep learning algorithm for
                            quantification of the idiopathic pulmonary fibrosis pattern</h3>
                        <p class="post-authors">

                            Tarun R Nimmada<sup>1</sup> ,

                            Pooja Rao<sup>1</sup> ,

                            Parang Sanghavi<sup>2</sup> ,

                            Vasanthakumar Venugopal<sup>3</sup> ,

                            Prashant Warier<sup>1</sup> ,

                            Zarir F Udwadia<sup>4</sup> ,

                            Bhavin Jankharia<sup>2</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. Jankharia Imaging Centre;&nbsp

                            3. Centre for Advanced Research in Imaging, Neurosciences and Genomics, New Delhi;&nbsp

                            4. Department of Pulmonology, Hinduja Hospital and Research Centre, Mumbai, India


                        </p>
                        <a href="/publications/2018/03/02/ild-ecr.html">
                            <p class="published-date">
                                Presented:
                                02 March 2018, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-8">
                        <label class="show-abstract" for="post-8"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>Radiologists are currently ill equipped to precisely estimate disease burden and track
                                the progression of idiopathic pulmonary fibrosis (IPF). Development of an automated
                                method for IPF segmentation is challenging, due to the complexity of the fibrosis
                                pattern and degree of variation between patients. Deep neural networks are machine
                                learning algorithms that overcome these challenges. We describe the development and
                                validation of a novel deep learning method to quantify the IPF pattern.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We used high-resolution chest CT scans from 23 patients with IPF as training data. The
                                fibrosis pattern was marked out on 60 slices per scan. Annotated scans, with 6
                                additional normal scans were used to train a convolutional neural network to outline the
                                IPF disease pattern. Segmentation accuracy was measured using Dice score. For each
                                patient, percentage of lungs affected by IPF was calculated. An independent set of 50
                                scans was used for clinical validation. Disease volume was independently estimated by 2
                                thoracic radiologists blinded to the algorithm estimate. Algorithm-derived estimates
                                were correlated with radiologist estimates of disease volume.</p>

                            <h4 id="results">Results</h4>
                            <p>A 3-dimensional neural network architecture coupled with 2-dimensional post-processing of
                                each slice produced the most accurate segmentation, with a Dice score of 0.77. The
                                correlation between algorithm-derived disease volume estimate and average radiologist
                                estimates was 0.92. Inter-radiologist correlation was 0.89. Radiologist estimates of
                                disease volume varied by 5.5% (range 0-15%).</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>We demonstrate that a deep neural network, trained using expert-annotated images, can
                                accurately quantify the percentage of lung volume affected by IPF.</p>

                            <a target="_blank" class="read-more"
                               href="http://ecronline.myesr.org/ecr2018/?p=recording&t=recorded&lecture=clinical-validation-of-a-deep-learning-algorithm-for-quantification-of-the-idiopathic-pulmonary-fibrosis-pattern">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-7" class="post-title">Automated detection and localisation of skull
                            fractures from CT scans using deep learning</h3>
                        <p class="post-authors">

                            Rohit Ghosh<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>1</sup> ,

                            Mustafa Biviji<sup>2</sup> ,

                            Pooja Rao<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. CT and MRI center, Nagpur


                        </p>
                        <a href="/publications/2018/03/02/fractures-ecr.html">
                            <p class="published-date">
                                Presented:
                                02 March 2018, European Congress of Radiology (ECR)
                            </p>
                        </a>
                        <input type="checkbox" id="post-7">
                        <label class="show-abstract" for="post-7"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>To develop and validate deep learning-based algorithm pipeline for fast detection and
                                localisation of skull fractures from non-contrast CT scans. All kinds of skull
                                fractures: undisplaced, depressed, comminuted, etc. were included as part of study.</p>

                            <h4 id="methods">Methods</h4>
                            <p>Anonymized and annotated dataset of 350 scans (11750 slices) with skull fractures were
                                used for generating candidate proposals for fractures. Stacked network pipeline was used
                                for candidate generation - a fully convolutional network for ROI generation and another
                                deep convolutional network for ROI classification. Final ROI classification model
                                (ResNet18) yielded fracture probabilities for candidates generated through the fully
                                convolutional (UNet) network. Separate deep learning model was trained to detect
                                haemorrhages on scan level which was used as proxy for clinical information. Fracture
                                candidate features like size, probabilities, depth for top 5 most probable fracture
                                candidates along with haemorrhage model confidence (phaemorrhage) were combined to train
                                random forest classifier to detect fracture on scan level. In case of predicted
                                fracture, most probable candidate(s) were used for localization.</p>

                            <h4 id="results">Results</h4>
                            <p>Separate set of 2971 scans, uniformly sampled from database with no exclusion criterion,
                                was used for testing scan-level decisions with 108 scans reported as skull fracture
                                cases. To evaluate scan-level decisions for fractures, area under receiver operating
                                curve (AUC-ROC) was calculated as 0.83 with phaemorrhage as feature and 0.72 without.
                                Free receiver operating curve yielded 0.9 sensitivity at 2.85 false-positives-per-scan.
                                Predictions on each patient takes &lt; 30s.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>Deep learning-based pipeline can accurately detect and localize skull fractures. Pipeline
                                can be used for triaging patients for presence of skull fractures.</p>


                            <a target="_blank" class="read-more"
                               href="http://ecronline.myesr.org/ecr2018/index.php?p=recording&t=recorded&lecture=automatic-detection-of-generalised-cerebral-atrophy-using-deep-neural-networks-from-head-ct-scans">
                                Watch recorded presentation at ECR (sign-up required) <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-6" class="post-title">Variation in practice patterns and outcomes across
                            United Network for Organ Sharing allocation regions</h3>
                        <p class="post-authors">

                            Dushyanth Srinivasan<sup>1</sup> ,

                            Benjamin Vaccaro<sup>1</sup> ,

                            Pooja Rao<sup>2</sup> ,

                            Rohit Ghosh<sup>2</sup> ,

                            Prashant Warier<sup>2</sup> ,

                            Tariq Ahmad<sup>1</sup> ,

                            Nihar R. Desai<sup>1,3</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Department of Cardiovascular Medicine, Yale School of Medicine, New Haven, CT;&nbsp

                            2. Qure.ai, Mumbai, India;&nbsp

                            3. Center for Outcomes Research, Yale University School of Medicine New Haven, CT


                        </p>
                        <a href="/publications/2018/01/22/organ-sharing.html">
                            <p class="published-date">
                                Published:
                                22 January 2018, Clinical Cardiology
                            </p>
                        </a>
                        <input type="checkbox" id="post-6">
                        <label class="show-abstract" for="post-6"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="background">Background</h4>
                            <p>The number of heart transplants performed is limited by organ availability and is managed
                                by the United Network for Organ Sharing (UNOS). Efforts are underway to make organ
                                disbursement more equitable as demand increases.</p>

                            <h4 id="hypothesis">Hypothesis</h4>
                            <p>Significant variation exists in contemporary patterns of care, wait times, and outcomes
                                among patients undergoing heart transplantation across UNOS regions.</p>

                            <h4 id="methods">Methods</h4>
                            <p>We identified adult patients undergoing first, single‐organ heart transplantation between
                                January 2006 and December 2014 in the UNOS dataset and compared sociodemographic and
                                clinical profiles, wait times, use of mechanical circulatory support (MCS), status at
                                time of transplantation, and 1‐year survival across UNOS regions.</p>

                            <h4 id="results">Results</h4>
                            <p>We analyzed 17 096 patients undergoing heart transplantation. There were no differences
                                in age, sex, renal function, and peripheral vascular resistance across regions; however,
                                there was 3‐fold variation in median wait time (range, 48–166 days) across UNOS regions.
                                Proportion of patients undergoing transplantation with status 1A ranged from 36% to 79%
                                across regions (P &lt; 0.01), and percentage of patients hospitalized at time of
                                transplantation varied from 41% to 98%. There was also marked variation in MCS and
                                inotrope utilization (28%–57% and 25%–58%, respectively; P &lt; 0.001). Durable
                                ventricular assist device implantation varied from 20% to 44% (P &lt; 0.001), and
                                intra‐aortic balloon pump utilization ranged from 4% to 18%.</p>

                            <h4 id="conclusions">Conclusions</h4>
                            <p>Marked differences exist in patterns of care across UNOS regions that generally trend
                                with differences in waitlist time. Novel policy initiatives are required to address
                                disparities in access to allografts and ensure equitable and efficient allocation of
                                organs.</p>

                            <a target="_blank" class="read-more"
                               href="https://onlinelibrary.wiley.com/doi/abs/10.1002/clc.22854"> Read full paper <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-5" class="post-title">Deep Neural Networks to Identify and Localize
                            Intracerebral Hemorrhage and Midline Shift in CT Scans of Brain</h3>
                        <p class="post-authors">

                            Mustafa Biviji<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>2</sup> ,

                            Rohit Ghosh<sup>2</sup> ,

                            Ankit Modi<sup>2</sup> ,

                            Pooja Rao<sup>2</sup> ,

                            Prashant Warier<sup>2</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Columbia Asia Radiology Group, Bengaluru;&nbsp

                            2. Qure.ai, Mumbai


                        </p>
                        <a href="/publications/2017/11/26/hemorrhages-rsna.html">
                            <p class="published-date">
                                Presented:
                                26 November 2017, Radiological Society of North America (RSNA)
                            </p>
                        </a>
                        <input type="checkbox" id="post-5">
                        <label class="show-abstract" for="post-5"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>CT scans of brain are often the frontline investigations in acute conditions of the
                                brain, particularly strokes. Treatment outcomes largely depend on a quick and accurate
                                interpretation of CT scans. A vital feature to illustrate severity of damage is midline
                                shift - indicating high intracerebral (IC) hemorrhage pressure, which can be fatal. Our
                                study aims at designing a deep convolutional network for detection, fast segmentation
                                and quantification of IC hemorrhage, devising an algorithm for midline shift measurement
                                and identification of cerebral hemisphere affected by the detected hemorrhage.</p>

                            <h4 id="methods">Methods</h4>
                            <p>The anonymized and annotated dataset had 39 CT scans of brain (16 with IC hemorrhage). A
                                deep neural network was trained slice by slice to segment hemorrhage. The network has
                                fully convolutional encoder and decoder with skip connections in between for better
                                localization. 26 scans (589 slices) were used for training and 13 scans (282 slices) for
                                validation. Features extracted from each patient’s complete IC hemorrhage segmentation
                                output were used to train a decision tree for final diagnosis. Ideal midline was drawn
                                using center of mass in bone window and anterior bone protrusion at the level of foramen
                                of Monro. This along with asymmetry in tissue densities gave displaced midline and
                                midline shift. Affected hemisphere was identified using displaced midline and
                                hemorrhage’s center of mass. Accuracy was measured using receiver operating
                                characteristic (ROC) curve and dice score.</p>

                            <h4 id="results">Results</h4>
                            <p>100 scans were separately collected over 2 weeks and used for testing. 6 of them had
                                hemorrhage. Sensitivity and specificity for the diagnosis of hemorrhage were 100% and
                                98.9% respectively. ROC analysis revealed area under curve of 0.994. Model took 3
                                seconds to segment one CT scan on average. The mean dice score for all test scans was
                                0.988 while it was 0.80 for the 6 scans with hemorrhage. Midline shift and affected
                                hemisphere were both identified with 100% accuracy.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>In this work, we trained a deep convolutional network to detect and quantify IC
                                hemorrhage in brain CT scans. We also measured midline shift and identified the affected
                                hemisphere. The processing pipeline was fully automatic.</p>

                            <h4 id="clinical-relevance">Clinical Relevance</h4>
                            <p>Automated detection of hemorrhage and midline shift can help rapidly distinguish between
                                ischemic and hemorrhagic strokes, enabling faster decision-making and treatment.</p>

                            <a target="_blank" class="read-more" href="https://rsna2017.rsna.org/program/index.cfm">
                                RSNA 2017 Program <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="conference">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-4" class="post-title">Generating Heatmaps to Visualize the Evidence of Deep
                            Learning Based Diagnosis of Chest X-Rays</h3>
                        <p class="post-authors">

                            Preetham Putha<sup>1</sup> ,

                            Manoj D Tadepalli<sup>1</sup> ,

                            Karthik Rao<sup>1</sup> ,

                            Shubham Jain<sup>1</sup> ,

                            Shalini Govil<sup>2</sup> ,

                            Prashant Warier<sup>1</sup> ,

                            Manojkumar K<sup>2</sup> ,

                            Namitha Sinha<sup>2</sup> ,

                            Manjunath KS<sup>2</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai;&nbsp

                            2. Columbia Asia Radiology Group, Bengaluru


                        </p>
                        <a href="/publications/2017/11/26/heatmaps-rsna.html">
                            <p class="published-date">
                                Presented:
                                26 November 2017, Radiological Society of North America (RSNA)
                            </p>
                        </a>
                        <input type="checkbox" id="post-4">
                        <label class="show-abstract" for="post-4"></label>
                        <span class="type-tag">conference</span>
                        <div class="post-entry texthide">
                            <h4 id="purpose">Purpose</h4>
                            <p>For radiologists to develop confidence in a deep learning diagnostic algorithm, it is
                                essential that the algorithm be able to visually demonstrate the evidence for the
                                diagnosis or disease tag. We describe the development of a method that highlights the
                                region(s) of a chest X-ray (CXR) responsible for a deep learning algorithm
                                diagnosis.</p>

                            <h4 id="methods">Methods</h4>
                            <p>Using 24,384 CXRs, we trained 18-layer deep residual convolutional neural networks to
                                predict if a chest X-ray was normal or abnormal, and to detect the presence of
                                ‘cardiomegaly, ‘opacity’, and ‘pleural effusion’ in a CXR. We then applied a method
                                called prediction difference analysis for visualization and interpretation of the
                                trained models. The contribution of each patch in the image is estimated as the degree
                                by which the prediction changes if that patch is replaced with an average normal patch.
                                This method was used to generate a relevance score for each pixel which is consequently
                                visualized as a heat map.</p>

                            <h4 id="results">Results</h4>
                            <p>We used a 60-20-20 split for train, validation and test sets. The trained neural network
                                showed an area under the ROC curve of 0.89, 0.92, 0.84, 0.91 for tagging abnormal,
                                cardiomegaly, opacity and pleural effusion respectively on the test set. The
                                visualization pipeline is used to generate heatmaps highlighting the enlarged heart,
                                opacities and the fluid corresponding to the cardiomegaly, opacity and pleural effusion
                                tags.</p>

                            <h4 id="conclusion">Conclusion</h4>
                            <p>We trained and tested a deep learning algorithm which accurately classifies and assigns
                                clinically relevant tags to CXRs. Further, we applied a visualization method that
                                generates heatmaps highlighting the most relevant parts of the CXR. The visualization
                                method is broadly applicable to other kinds of X-rays, and to other deep learning
                                algorithms. Future work will focus on formally validating the accuracy of the
                                visualization, by measuring overlap between radiologist annotation and
                                algorithm-generated heatmap.</p>

                            <h4 id="clinical-relevance">Clinical Relevance</h4>
                            <p>Heatmaps highlighting evidence for disease tags will provide clinical users with crucial
                                visual cues that could ease their decision to accept or reject a deep learning based
                                chest x-ray diagnosis.</p>

                            <a target="_blank" class="read-more" href="https://rsna2017.rsna.org/program/index.cfm">
                                RSNA 2017 Program <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-3" class="post-title">2D-3D Fully Convolutional Neural Networks for Cardiac
                            MR Segmentation</h3>
                        <p class="post-authors">

                            Jay Patravali<sup>1</sup> ,

                            Shubham Jain<sup>1</sup> ,

                            Sasank Chilamkurthy<sup>1</sup>

                        </p>
                        <p class="author-affiliations">

                            1. Qure.ai, Mumbai


                        </p>
                        <a href="/publications/2017/07/31/cardiac-segmentation.html">
                            <p class="published-date">
                                Published:
                                31 July 2017
                            </p>
                        </a>
                        <input type="checkbox" id="post-3">
                        <label class="show-abstract" for="post-3"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="abstract">Abstract</h4>

                            <p>In this paper, we develop a 2D and 3D segmentation pipelines for fully automated cardiac
                                MR image segmentation using Deep Convolutional Neural Networks (CNN). Our models are
                                trained end-to-end from scratch using the ACD Challenge 2017 dataset comprising of 100
                                studies, each containing Cardiac MR images in End Diastole and End Systole phase. We
                                show that both our segmentation models achieve near state-of-the-art performance scores
                                in terms of distance metrics and have convincing accuracy in terms of clinical
                                parameters. A comparative analysis is provided by introducing a novel dice loss function
                                and its combination with cross entropy loss. By exploring different network structures
                                and comprehensive experiments, we discuss several key insights to obtain optimal model
                                performance, which also is central to the theme of this challenge.</p>


                            <a target="_blank" class="read-more" href="http://jaha.ahajournals.org/content/7/8/e008081">
                                Read full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-2" class="post-title">Improving Boundary Classification for Brain Tumor
                            Segmentation and Longitudinal Disease Progression</h3>
                        <p class="post-authors">

                            Ramandeep Randhawa<sup>1</sup> ,

                            Ankit Modi<sup>2</sup> ,

                            Parag Jain<sup>3</sup> ,

                            Prashant Warier<sup>3</sup>

                        </p>
                        <p class="author-affiliations">

                            1. University of Southern California, Los Angeles, USA;&nbsp

                            2. Qure.ai, Mumbai;&nbsp

                            3. Dhristi Inc., Palo Alto, USA


                        </p>
                        <a href="/publications/2017/04/12/boundary-classification-brain-tumor.html">
                            <p class="published-date">
                                Published:
                                12 April 2017
                            </p>
                        </a>
                        <input type="checkbox" id="post-2">
                        <label class="show-abstract" for="post-2"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="abstract">Abstract</h4>

                            <p>Tracking the progression of brain tumors is a challenging task, due to the slow growth
                                rate and the combination of different tumor components, such as cysts, enhancing
                                patterns, edema and necrosis. In this paper, we propose a Deep Neural Network based
                                architecture that does automatic segmentation of brain tumor, and focuses on improving
                                accuracy at the edges of these different classes. We show that enhancing the loss
                                function to give more weight to the edge pixels significantly improves the neural
                                network’s accuracy at classifying the boundaries. In the BRATS 2016 challenge, our
                                submission placed third on the task of predicting progression for the complete tumor
                                region.</p>

                            <a target="_blank" class="read-more"
                               href="https://link.springer.com/chapter/10.1007/978-3-319-55524-9_7"> Read full paper <i
                                    class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


            <article class="post mr-lg-auto ml-lg-auto col-12 col-lg-10" data-type="journal">
                <div class="post-inner">
                    <div class="content">
                        <h3 id="publication-1" class="post-title">A Deep-Learning Based Approach for Ischemic Stroke
                            Lesion Outcome Prediction</h3>
                        <p class="post-authors">

                            Ramandeep Randhawa<sup>1</sup> ,

                            Ankit Modi<sup>2</sup> ,

                            Parag Jain<sup>3</sup> ,

                            Prashant Warier<sup>3</sup>

                        </p>
                        <p class="author-affiliations">

                            1. University of Southern California, Los Angeles, USA;&nbsp

                            2. Qure.ai, Mumbai;&nbsp

                            3. Dhristi Inc., Palo Alto, USA


                        </p>
                        <a href="/publications/2016/10/17/ischemic-stroke-lesion-outcome-prediction.html">
                            <p class="published-date">
                                Published:
                                17 October 2016, ISLES 2016 proceedings
                            </p>
                        </a>
                        <input type="checkbox" id="post-1">
                        <label class="show-abstract" for="post-1"></label>
                        <span class="type-tag">journal</span>
                        <div class="post-entry texthide">
                            <h4 id="abstract">Abstract</h4>

                            <p>The ISLES 2016 challenge aims to address two important aspects of Ischemic stroke lesion
                                treatment prediction. The first aspect relates to segmenting the brain MRI to identify
                                the areas with lesions and the second aspect relates to predicting the actual clinical
                                outcome in terms of the patient’s degree of disability. The input data consists of acute
                                MRI scans and additional clinical such as TICI scores, Time Since Stroke, and Time to
                                Treatment.
                                To address this challenge we take a deep-learning based approach. In particular, we
                                first focus on the segmentation task and use an automatic segmentation model that
                                consists of a Deep Neural Network (DNN). The DNN takes as input the MRI images and
                                outputs the segmented image, automatically learning the latent underlying features
                                during the training process. The DNN architectures we consider utilize many
                                convolutional layers with small kernels, e.g., 3x3. This approach requires fewer
                                parameters to estimate, and allows one to learn and generalize from the somewhat limited
                                amount of data that is provided.
                                One of the architectures we are currently utilizing is based on the U-Net [1], which is
                                an all-convolutional network. It acts as an auto-encoder, that first “en- codes” the
                                input image by applying combinations of convolutional and pooling operations. This is
                                followed by the “decoding” step that up-scales the encoded images, while performing
                                convolutions. The all-convolutional architecture of the U-Net allows it to handle input
                                images of different dimensions as in the challenge dataset. In our experiments, we found
                                that this architecture yielded excellent performance on the previous ISLES 2015 dataset.
                                Although the modalities in the 2016 challenge are different, our initial training
                                experiments have yielded promising segmentation results.
                                Our next steps involve addressing the regression challenge. There is limited amount of
                                labeled data for this task. Our approach will be to include these outcomes as part of
                                the segmentation training directly. This will allow the DNN to learn latent features
                                that can directly help with the classification task.</p>

                            <a target="_blank" class="read-more"
                               href="http://www.isles-challenge.org/ISLES2016/pdf/20160927_ISLES2016_Proceedings.pdf">
                                Read full paper <i class="fa fa-long-arrow-right"></i></a>
                        </div>
                    </div><!--//content-->
                </div><!--//post-inner-->
            </article><!--//post-->


        </div>
    </div>
</div>

<script type="text/javascript">
    var $articles = $('.blog-list article');
    var $journals = $articles.filter('article[data-type="journal"]')
    var $conference = $articles.filter('article[data-type="conference"]');

    var $filterButtons = $('.filter-container .btn');
    $filterButtons.on('click', function (e) {
        var $el = $(this);
        var type = $el.attr('data-type');

        if (type === 'journal') {
            $conference.fadeOut();
            $journals.fadeIn();
        }

        if (type === 'conference') {
            $journals.fadeOut();
            $conference.fadeIn();
        }

        if (type === 'all') $articles.fadeIn();

        if ($el.hasClass('active')) return;
        $filterButtons.removeClass('active');
        $el.addClass('active');

    })
</script>

<style type="text/css">
    .filter-container {
        display: flex;
        align-items: center;
        justify-content: flex-end;
    }

    .filter-container button {
        margin-left: 0.5rem;
    }

    .filter-container button:hover {
        background: #f0f4f7;
    }

    .filter-container button.active {
        background: #0094c6;
        color: white;
    }

    .filter-container button.active:hover {
        background: #05a4da;
    }

    .type-tag {
        display: inline-block;
        float: right;
        text-transform: capitalize;
        padding: 0.25rem 1.25rem;
        background: #4d96c1;
        color: white;
        border-radius: 4px;
        font-size: 12px;
        letter-spacing: 1px;
        font-weight: bold;
    }

</style>


<!-- ******FOOTER****** -->
<footer class="footer">
    <div class="footer-content">
        <div class="container">
            <div class="row">
                <div class="footer-col links col-sm-2 col-4">
                    <div class="footer-col-inner">
                        <h3 class="title">Products</h3>
                        <ul class="list-unstyled">
                            <li><a href="/#why">qXR</a></li>
                            <li><a href="/#qER">qER</a></li>
                            <li><a href="/#qQuant">qQuant</a></li>
                            <li><a href="https://scan.qure.ai">Free Trial<sup>*</sup></a></li>
                        </ul>
                    </div><!--//footer-col-inner-->
                </div><!--//foooter-col-->

                <div class="footer-col links col-sm-2  col-4">
                    <div class="footer-col-inner">
                        <h3 class="title">Research</h3>
                        <ul class="list-unstyled">
                            <li><a href="/research_highlights.html">Highlights</a></li>
                            <li><a href="/publications.html">Publications</a></li>
                            <li><a href="http://blog.qure.ai">Blog</a></li>
                        </ul>
                    </div><!--//footer-col-inner-->
                </div><!--//foooter-col-->

                <div class="footer-col links col-sm-2 col-4">
                    <div class="footer-col-inner">
                        <h3 class="title">About</h3>
                        <ul class="list-unstyled">
                            <li><a href="about.html">Team</a></li>
                            <li><a href="news.html">News</a></li>
                            <li><a href="security-and-regulatory.html">Security and Regulatory</a></li>
                            <li><a href="privacy-policy.html">Privacy Policy</a></li>
                        </ul>
                    </div><!--//footer-col-inner-->
                </div><!--//foooter-col-->

                <div class="footer-col connect col-sm-6 col-12">
                    <div class="footer-col-inner">
                        <ul class="social list-inline">
                            <li class="list-inline-item"><a href="https://twitter.com/qure_ai" target="_blank"><i
                                    class="fa fa-twitter"></i></a></li>
                            <li class="list-inline-item"><a href="https://www.linkedin.com/company/qure.ai/"><i
                                    class="fa fa-linkedin"></i></a></li>
                            <li class="list-inline-item"><a href="https://github.com/qureai"><i
                                    class="fa fa-github-alt"></i></a></li>
                        </ul>
                        <div class="form-container">
                            <p class="intro">Stay up to date with the latest news from Qure.ai</p>
                            <div id="mc_embed_signup">
                                <form action="https://qure.us15.list-manage.com/subscribe/post?u=f479ab0e89e5c5edf68c1bc07&amp;id=1a00296d4c"
                                      method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form"
                                      class="validate signup-form navbar-form" target="_blank" novalidate>
                                    <div id="mc_embed_signup_scroll">
                                        <div class="form-group">
                                            <input type="email" value class="form-control" name="EMAIL" id="mce-EMAIL"
                                                   placeholder="Enter your email" required>
                                            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
                                            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input
                                                    type="text" name="b_f479ab0e89e5c5edf68c1bc07_1a00296d4c"
                                                    tabindex="-1" value></div>
                                            <div class="clear"><input type="submit" value="Subscribe" name="subscribe"
                                                                      id="mc-embedded-subscribe"
                                                                      class="btn btn-cta btn-cta-primary"></div>
                                        </div>
                                    </div>
                                </form>
                            </div>
                        </div>
                        <!--End mc_embed_signup-->
                    </div><!--//footer-col-inner-->
                </div><!--//foooter-col-->
            </div>
            <div class="row">
                <div class="footer-col col-12">
                    <div class="footer-col-inner">
                        <p> *qER is cleared by FDA for triage and notification. qXR is available for investigational use
                            only, in the United States </p>
                    </div><!--//footer-col-inner-->
                </div>
                <div class="row">
                    <div class="footer-col contact col-12">
                        <div class="footer-col-inner">
                            <h3 class="title">Contact Us</h3>
                            <ul class="list-unstyled list-inline">
                                <li class="list-inline-item"><i class="fa fa-map-marker"></i>Level 6, Oberoi Commerz II,
                                    Goregaon East, Mumbai 400063, India
                                </li>
                                <li class="list-inline-item"><i class="fa fa-map-marker"></i>Suite 76J, One World Trade
                                    Center, New York, NY 10007
                                </li>
                                <!-- <li class="list-inline-item"><i class="fa fa-phone" aria-hidden="true"></i> +91 22 4067 5800</a></li> -->
                                <li class="list-inline-item"><a href="mailto:partner@qure.ai"><i
                                        class="fa fa-envelope-o"></i> partner@qure.ai</a></li>
                            </ul>
                        </div><!--//footer-col-inner-->
                    </div><!--//contact-->

                    <div class="clearfix"></div>
                </div><!--//row-->


            </div><!--//container-->
        </div><!--//footer-content-->
    </div>
</footer><!--//footer-->


<!-- Javascript -->
<script src="js/bootstrap.min.js" crossorigin="anonymous"></script>
<script type="text/javascript" src="js/jquery.flexslider-min.js"></script>
<script type="text/javascript" src="js/popper.min.js"></script>
<script type="text/javascript" src="js/back-to-top.js"></script>
<script type="text/javascript" src="js/main.js"></script>

<script>
    $(document).ready(function () {
        var secname = window.location.hash.substr(1);
        console.log(secname);
        if (secname !== "") {
            $('html,body').animate({
                scrollTop: $("#" + secname).offset().top - 55
            }, 1000);
        }
    });

</script>

<!-- Global site tag (gtag.js) - Google Ads: 758876722 -->
<script async src="js_1"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'AW-758876722');
</script>

<!-- Modal video -->
<script type="text/javascript" src="js/modal-video.js"></script>

</div>
</body>

</html>
